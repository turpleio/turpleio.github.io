<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=3">
<meta httpequiv="x-ua-compatible" content="ie=edge">
<meta property="og:type" content="website">
<meta name="author" content="turpleio">
<meta name="generator" content="Docusaurus v2.0.0-alpha.75">
<link href="https://www.googletagmanager.com" rel="dns-prefetch">
<link href="https://www.google-analytics.com" rel="dns-prefetch">
<link rel="icon" href="/favicon.png">
<link rel="apple-touch-icon" href="/img/icons/apple-180x180.png" sizes="180x180">
<meta name="msapplication-config" content="/browserconfig.xml">
<link rel="sitemap" type="application/xml" href="/sitemap.xml">
<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=GTM-PVR7M2G"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","GTM-PVR7M2G",{anonymize_ip:!0})</script>
<link rel="search" type="application/opensearchdescription+xml" title="Turple: Cloud Native Platform" href="/opensearch.xml">
<link rel="manifest" href="/manifest.webmanifest">
<meta name="theme-color" content="#d14671">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#21222c">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Turple: Cloud Native Platform Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Turple: Cloud Native Platform Blog Atom Feed"><title data-react-helmet="true">Importing 300k rows/sec with io_uring | Turple: Cloud Native Platform</title><meta data-react-helmet="true" property="og:title" content="Importing 300k rows/sec with io_uring | Turple: Cloud Native Platform"><meta data-react-helmet="true" name="description" content="QuestDB 6.5 introduces a new `COPY` commands allowing importing large CSV files. This article reveals the story behind it and highlights the exciting benchmark results using this new SQL command."><meta data-react-helmet="true" name="twitter:description" content="QuestDB 6.5 introduces a new `COPY` commands allowing importing large CSV files. This article reveals the story behind it and highlights the exciting benchmark results using this new SQL command."><meta data-react-helmet="true" property="og:description" content="QuestDB 6.5 introduces a new `COPY` commands allowing importing large CSV files. This article reveals the story behind it and highlights the exciting benchmark results using this new SQL command."><meta data-react-helmet="true" name="twitter:title" content="Importing 300k rows/sec with io_uring | Turple: Cloud Native Platform"><meta data-react-helmet="true" name="twitter:image:alt" content="Image for &quot;Importing 300k rows/sec with io_uring | Turple: Cloud Native Platform&quot;"><meta data-react-helmet="true" name="keywords" content="io_uring,benchmark,questdb,time series"><meta data-react-helmet="true" property="og:image" content="https://turpleio.github.io/img/blog/2022-09-12/cover.png"><meta data-react-helmet="true" name="twitter:image" content="https://turpleio.github.io/img/blog/2022-09-12/cover.png"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><link rel="stylesheet" href="/assets/css/styles.4f6557e4.css">
</head>
<body itemscope itemtype="http://schema.org/Organization">
<meta itemprop="name" content="Turple: Cloud Native Platform">
<meta itemprop="description" content="Turple is a cloud native platform which can help you build and operate your services on cloud.">
<meta itemprop="url" content="https://turpleio.github.io">
<meta itemprop="logo" content="https://turpleio.github.io/img/favicon.png">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"dark")}()</script><div id="__docusaurus">
<nav class="navbar navbar_wBc8 navbar--light"><div class="navbar__inner inner_Y8Z6"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand brand_AtiA" href="/">QuestDB</a></div><div class="navbar__items navbar__items--right"></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand brand_AtiA" href="/">QuestDB</a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"></ul></div></div></div></nav><div class="wrapper_DIJ0 blog-wrapper"><div class="container margin-vert--lg"><div class="row"><div class="col col--3"><div class="sidebar_q+wC thin-scrollbar"><h3 class="sidebarItemTitle_9G5K">Recent posts</h3><ul class="sidebarItemList_6T4b"><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/2022/12/19/merry-questmas-gifts-2023">Merry Questmas! Here are your gifts for 2023...</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/2022/12/13/using-prometheus-loki-grafana-monitor-questdb-kubernetes">Using Prometheus, Loki, and Grafana to monitor QuestDB in Kubernetes</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/2022/11/30/full-table-scan-are-fast">Listen to Your CPU - Full-table Scans Are Fast</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/2022/11/25/questdb-6.6.1-dynamic-commits">QuestDB 6.6.1 - Dynamic Commits</a></li><li class="sidebarItem_cjdF"><a class="sidebarItemLink_zyXk" href="/blog/2022/11/23/sql-extensions-time-series-data-questdb-part-ii">SQL Extensions for Time Series Data in QuestDB - Part II</a></li></ul></div></div><main class="col col--7"><article><header><h1 class="title_KBQu">Importing 300k rows/sec with io_uring</h1><time datetime="2022-09-12T00:00:00.000Z" class="date_A31P">September 12, 2022 · 13 min read</time><div class="avatar margin-vert--md"><a href="https://github.com/puzpuzpuz" target="_blank" rel="noopener noreferrer" class="avatar__photo-link avatar__photo"><img src="https://avatars.githubusercontent.com/puzpuzpuz" alt="Andrey Pechkurov"></a><div class="avatar__intro"><h4 class="avatar__name"><a href="https://github.com/puzpuzpuz" target="_blank" rel="noopener noreferrer">Andrey Pechkurov</a></h4><small class="avatar__subtitle">QuestDB Engineering</small></div></div></header><div class="markdown"><p>In this blog post, QuestDB’s very own
<a href="https://github.com/puzpuzpuz" target="_blank" rel="noopener noreferrer">Andrei Pechkurov</a> presents how to ingest large
CSV files a lot more efficiently using the SQL
<a href="https://questdb.io/docs/reference/sql/copy" target="_blank" rel="noopener noreferrer"><code>COPY</code></a> statement, and takes us
through the journey of benchmarking. Andrei also shares insights about how the
new improvement is made possible by <code>io_uring</code> and compares QuestDB&#x27;s import
versus several well-known OLAP and time-series databases in Clickhouse&#x27;s
ClickBench benchmark.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="introduction"></a>Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">#</a></h2><p>As an open source time series database company, we understand that getting your
existing data into the database in a fast and convenient manner is as important
as being able to <a href="https://questdb.io/time-series-benchmark-suite" target="_blank" rel="noopener noreferrer">ingest</a> and
<a href="https://questdb.io/blog/2022/05/26/query-benchmark-questdb-versus-clickhouse-timescale" target="_blank" rel="noopener noreferrer">query</a>
your data efficiently later on. That&#x27;s why we decided to dedicate our new
release, QuestDB 6.5, to the new parallel
<a href="https://questdb.io/docs/guides/importing-data" target="_blank" rel="noopener noreferrer">CSV file import</a> feature. In
this blog post, we discuss what parallel import means for our users and how it&#x27;s
implemented internally. As a bonus, we also share how recent ClickHouse team&#x27;s
benchmark helped us to improve both QuestDB and its demonstrated results.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="how-clickbench-helped-us-improve"></a>How ClickBench helped us improve<a class="hash-link" href="#how-clickbench-helped-us-improve" title="Direct link to heading">#</a></h2><p>Recently ClickHouse conducted a
<a href="https://github.com/ClickHouse/ClickBench" target="_blank" rel="noopener noreferrer">benchmark</a> for their own database and
many others, including QuestDB. The benchmark included data import as the first
step. Since we were in the process of building a faster import, this benchmark
provided us with nice test data and baseline results. So, what have we achieved?
Let&#x27;s find out. The benchmark was using QuestDB&#x27;s HTTP
<a href="https://questdb.io/docs/reference/api/rest#imp---import-data" target="_blank" rel="noopener noreferrer">import endpoint</a>
to ingest the data into an existing non-partitioned table. You may wonder why it
doesn&#x27;t use a <a href="https://questdb.io/docs/concept/partitions" target="_blank" rel="noopener noreferrer">partitioned</a> table,
which stores the data sorted by the timestamp values and provides many benefits
for time series analysis. Most likely, the reason is terrible import execution
time. Both HTTP-based import and pre-6.5 COPY SQL command are simply not capable
of importing a big CSV file with unsorted data. Thus, the benchmark opts for a
non-partitioned table with no designated timestamp column. The test CSV file may
be downloaded and uncompressed following the commands:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI bash"><div tabindex="0" class="prism-code language-bash codeBlock_rtdJ thin-scrollbar"><div class="codeBlockLines_1zSZ" style="color:#f8f8f2;background-color:#262833"><div class="token-line" style="color:#f8f8f2"><span class="token function" style="color:#8be9fd">wget</span><span class="token plain"> </span><span class="token string" style="color:#f1fa8c">&#x27;https://datasets.clickhouse.com/hits_compatible/hits.csv.gz&#x27;</span><span class="token plain"></span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain"></span><span class="token function" style="color:#8be9fd">gzip</span><span class="token plain"> -d hits.csv.gz</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB">Copy</button></div></div><p>The file is on the bigger side, 76GB when decompressed, and contains rows that
are heavily out-of-order in terms of time. This makes it a nice import
performance challenge for any time series database. Getting the data into a
locally running QuestDB instance via HTTP is as simple as:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI bash"><div tabindex="0" class="prism-code language-bash codeBlock_rtdJ thin-scrollbar"><div class="codeBlockLines_1zSZ" style="color:#f8f8f2;background-color:#262833"><div class="token-line" style="color:#f8f8f2"><span class="token function" style="color:#8be9fd">curl</span><span class="token plain"> -F </span><span class="token assign-left variable" style="color:#ff79c6">data</span><span class="token operator" style="color:#ff79c6">=</span><span class="token plain">@hits.csv </span><span class="token string" style="color:#f1fa8c">&#x27;http://localhost:9000/imp?name=hits&#x27;</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB">Copy</button></div></div><p>Such import took almost 28 minutes (1,668 seconds, to be precise) on a
c6a.4xlarge EC2 instance with a 500GB gp2 volume in ClickBench. This yields
around 47MB/s and leaves a lot to wish for. In contrast, it took ClickHouse
database around 8 minutes (476 seconds) to import the file on the same hardware.
But since we were already working on faster imports for partitioned tables, this
benchmark provided us with nice test data and baseline results.</p><p>In addition to import speed, ClickBench measures query performance. Although
none of the queries it ran were related to time series analysis, the results
helped us to improve QuestDB. We found and fixed a stability issue, as well as
added support for some SQL functions. Other than that, our SQL engine had a bug
around multi-threaded <code>min()</code>/<code>max()</code> SQL function optimization: it was
case-sensitive and simply ignored <code>MIN()</code>/<code>MAX()</code> used in ClickBench. After a
trivial fix, queries using these aggregate functions got their intended speed
back. Finally, a few queries marked with N/A result were using unsupported SQL
syntax and it was trivial to rewrite them to get proper results. With all of
these improvements, we have run ClickBench on QuestDB 6.5.2 and created a
<a href="https://github.com/ClickHouse/ClickBench/pull/25" target="_blank" rel="noopener noreferrer">pull request</a> with the
updated results.</p><p>Long story short, although ClickBench has nothing to do with time series
analysis, it provided us with a test CSV file and baseline import results, as
well as helped us to improve query stability and performance.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="the-import-speed-up"></a>The import speed-up<a class="hash-link" href="#the-import-speed-up" title="Direct link to heading">#</a></h2><p>Our new optimized import is based on the SQL <code>COPY</code> statement:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI questdb-sql"><div tabindex="0" class="prism-code language-questdb-sql codeBlock_rtdJ thin-scrollbar"><div class="codeBlockLines_1zSZ" style="color:#f8f8f2;background-color:#262833"><div class="token-line" style="color:#f8f8f2"><span class="token plain">COPY hits FROM &#x27;hits.csv&#x27; WITH TIMESTAMP &#x27;EventTime&#x27; FORMAT &#x27;yyyy-MM-dd HH:mm:ss&#x27;;</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB">Copy</button></div></div><p>The above command uses the new <code>COPY</code> syntax to import the <code>hits.csv</code> file from
ClickBench to the <code>hits</code> table. For the command to work, the file should be made
available in the import root directory configured on the server:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI bash"><div tabindex="0" class="prism-code language-bash codeBlock_rtdJ thin-scrollbar"><div class="codeBlockLines_1zSZ" style="color:#f8f8f2;background-color:#262833"><div class="token-line" style="color:#f8f8f2"><span class="token plain">cairo.sql.copy.root</span><span class="token operator" style="color:#ff79c6">=</span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain">home</span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain">my-user</span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain">my-qdb-import</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB">Copy</button></div></div><p>Since we care about time series data analysis, in our experiments, we
partitioned it by day while the original benchmark used a non-partitioned table.
Let&#x27;s start with the most powerful AWS EC2 instance from the original benchmark:</p><figure><img alt="Bar chart showing import comparison. From fast to slow: ClickHouse, QuestDB, Apache Pinot, TimescaleDB, DuckDB, and Apache Druid." class="image_Pw1y margin_Rc0b shadow_7z-4 title_f-p+" height="360" src="/img/blog/2022-09-12/cover.png" width="650"><figcaption class="caption_-tGK">Ingesting a 76GB CSV file, from fast to slow: ClickHouse, QuestDB, Apache Pinot, TimescaleDB, DuckDB, and Apache Druid.</figcaption></figure><p>The above benchmark compares the import speed of several well-known OLAP and
time-series databases: Apache Pinot, Apache Druid, ClickHouse, DuckDB,
TimescaleDB, and QuestDB. Here, our new optimized <code>COPY</code> imports almost 100M
rows from the <code>hits.csv</code> file in 335 seconds, leaving a higher place in the
competition only to ClickHouse.</p><p>We also did a run on the c6a.4xlarge instance (16 vCPU and 32GB RAM) from the
original benchmark which is noticeably less powerful than the c6a.metal instance
(192 vCPU and 384GB RAM). Yet, both instances had a rather slow gp2 500GB EBS
volume, the result was 17,401 seconds for the less powerful c6a.4xlarge
instance. So, in spite of a very slow disk, c6a.metal is 52x faster than
c6a.4xlarge. Why is that?</p><p>The answer is simple. The metal instance has a huge amount of memory, so once
the CSV file gets decompressed, it fits fully into the OS page cache. Hence, the
import doesn&#x27;t do any physical reads from the input file and instead reads the
pages from the memory (note: the machine has a
<a href="https://en.wikipedia.org/wiki/Non-uniform_memory_access" target="_blank" rel="noopener noreferrer">NUMA</a> architecture,
but non-local memory access is still way faster than the disk reads). That&#x27;s why
we observe such huge difference here for QuestDB and, also, you may notice a
2.5x difference for ClickHouse in the original benchmark.</p><p>You may wonder why, by removing the need to read the data from the slow disk,
QuestDB makes a very noticeable improvement, while it&#x27;s only 2.5x for ClickHouse
and even less for other databases? We&#x27;re going to explain it soon, but for now,
let&#x27;s continue the benchmarking fun.</p><p>Honestly speaking, we find the choice of the metal instance in the ClickBench
results rather synthetic, as it makes little sense to use a very powerful (and
expensive) machine in combination with a very slow (and cheap) disk. So, we did
a benchmark run on a different test stand:</p><ul><li>c5d.4xlarge EC2 instance (16 vCPU and 32GB RAM), Amazon Linux 2 with 5.15.50
kernel</li><li>400GB NVMe drive</li><li>250GB gp3, 16K IOPS and 1GB/s throughput, or gp2 of the same size</li></ul><p>What we got is the following:</p><figure><img alt="Bar chart showing QuestDB import performance using different HW." class="image_Pw1y margin_Rc0b shadow_7z-4 title_f-p+" height="360" src="/img/blog/2022-09-12/comparison.png" width="650"><figcaption class="caption_-tGK">QuestDB ingestion time for ClickBench&#x27;s 76GB CSV file by instance type and storage.</figcaption></figure><p>The very last result on the above chart stands for the scenario of c5d.4xlarge
instance with a slow gp2 volume. We are including it to show the importance of
the disk speed to the performance.</p><p>In the middle of the chart, the-gp3-volume-only result doesn&#x27;t use the local
SSD, but manages to ingest the data into a partitioned table a lot faster than
the gp2 run, thanks to the faster EBS volume. Finally, in the NVMe SSD run, the
import takes less than 7 minutes - an impressive ingestion rate of 248,000 row/s
(or 193MB/s) without having the whole input file in the OS page cache. Here, the
SSD is used as a read-only storage for the CSV file, while the database files
are placed on the EBS volume. This is a convenient approach for a single-time
import of high volume of data. As soon as the import is done, the SSD is no
longer needed, so the EBS volume may be attached to a more affordable instance
where the database would run.</p><p>As shown by the top result in the chart above, the optimized import makes a
terrific difference for anyone who wants to import their time series data to
QuestDB, but also takes us close to the ClickHouse&#x27;s results from the practical
perspective. Another nice property of QuestDB&#x27;s import is that, as soon as the
import ends, the data is laid out on disk optimally, i.e. the column files are
organized in partitions and no background merging is required.</p><p>Now, as we promised, we&#x27;re going to explain why huge amount of RAM or a
locally-attached SSD makes such a difference for QuestDB&#x27;s import performance.
To learn that, we&#x27;re taking a leap into an engineering story full of trial and
error.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="optimizing-the-import"></a>Optimizing the import<a class="hash-link" href="#optimizing-the-import" title="Direct link to heading">#</a></h2><p>Our HTTP endpoint, as well as the old <code>COPY</code> implementation, is handling the
incoming data serially (think, as a single-time stream) and uses a single thread
for that. For out-of-order (O3) data, this means lots of O3 writes and, hence,
partition re-writes. Both single-threaded handling and O3 writes become the
limiting factor for these types of import.</p><p>However, the <code>COPY</code> statement operates on a file, so there is nothing preventing
us from going over it as many times as needed.</p><p>QuestDB&#x27;s storage format doesn&#x27;t involve complicated layout like the one in
<a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree" target="_blank" rel="noopener noreferrer">LSM trees</a> or in other
similar persistent data structures. The column files are
<a href="https://questdb.io/docs/concept/partitions" target="_blank" rel="noopener noreferrer">partitioned</a> by time and versioned
to handle concurrent reads and writes. The advantages of this approach is that
as soon as the rows are committed, the on-disk data is optimal from the read
operation perspective - there is no need to go through multiple files with
potentially overlapping data when reading from a single partition. The downside
is that such storage format may be problematic to cope with, when it comes to
data import.</p><p>But no worries, that&#x27;s something we have optimized.</p><p>The big ideas we had when working on our shiny new <code>COPY</code> are really simple.
First, we should organize the import in multiple phases in order to enable
in-order data ingestion. Second, we go parallel, i.e. multi-threaded, in each of
those phases, where it is possible.</p><p>Broadly speaking, the phases are:</p><ol><li>Check input file boundaries. Here we try to split the file into N chunks, so
that N worker threads may work on their own chunk in isolation.</li><li>Index the input file. Each thread scans its chunk, reads designated timestamp
column values, and creates temporary index files. The index files are
organized in partitions and contain sorted timestamps, as well as offsets
pointing to the source file.</li><li>Scan the input file and import data into temporary tables. Here, the threads
use the newly built indexes to go through the input file and write their own
temporary tables. The scanning and subsequent writes are guaranteed to be
in-order thanks to the index files containing timestamps and offsets tuples
sorted by time. The parallelism in this phase comes from multiple partitions
being available to the threads to work independently.</li><li>Perform additional metadata manipulations (say, merge symbol tables) and,
finally, move the partitions from temporary tables to the final one. This is
completed in multiple smaller phases that we summarize as one, for the sake
of simplicity.</li></ol><p>The indexes we build at phase 2 may be illustrated in the following way:</p><figure><img alt="A diagram showing temporary indexes built during parallel import." class="image_Pw1y margin_Rc0b shadow_7z-4 title_f-p+" height="265" src="/img/blog/2022-09-12/diagram.png" width="700"><figcaption class="caption_-tGK">Temporary indexes built during parallel import.</figcaption></figure><p>The above description is an overview of what we&#x27;ve done for the new <code>COPY</code>. Yet,
a careful reader might spot a potential bottleneck. Yes, the third phase
involves lots of random disk reads in case of an unordered input file. That&#x27;s
exactly what we observed as a noticeable bottleneck when experimenting with the
initial implementation. But does it mean that there is nothing we can do with
this? Not really. Modern HW &amp; SW to the rescue!</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="io_uring-everything"></a>io_uring everything!<a class="hash-link" href="#io_uring-everything" title="Direct link to heading">#</a></h2><p>Modern SSDs, especially NVMe ones, have evolved quite far from their spinning
magnetic ancestors. They&#x27;re able to cope with much higher concurrency levels for
disk operations, including random read ones. But utilizing these hardware
capabilities with traditional blocking interfaces, like
<a href="https://man7.org/linux/man-pages/man2/pwrite.2.html" target="_blank" rel="noopener noreferrer"><code>pread()</code></a>, would involve
many threads and, hence, some overhead here and there (like increased memory
footprint or context switching). Moreover, QuestDB&#x27;s threading model operates on
a fixed-size thread pool and doesn&#x27;t assume running more threads than the
available CPU cores.</p><p>Luckily, newer Linux kernel versions support
<a href="https://kernel.dk/io_uring.pdf" target="_blank" rel="noopener noreferrer"><code>io_uring</code></a>, a new asynchronous I/O interface.
But would it help in our case? Learning the answer is simple and, in fact,
doesn&#x27;t even require a single line of code, thanks to
<a href="https://github.com/axboe/fio" target="_blank" rel="noopener noreferrer">fio</a>, a very flexible I/O tester utility.</p><p>Let&#x27;s check how blocking random reads of 4KB chunks would perform on a laptop
with a decent NVMe SSD:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI bash"><div tabindex="0" class="prism-code language-bash codeBlock_rtdJ thin-scrollbar"><div class="codeBlockLines_1zSZ" style="color:#f8f8f2;background-color:#262833"><div class="token-line" style="color:#f8f8f2"><span class="token plain">$ fio --name</span><span class="token operator" style="color:#ff79c6">=</span><span class="token plain">read_sync_4k </span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain"></span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">      --filename</span><span class="token operator" style="color:#ff79c6">=</span><span class="token plain">./hits.csv </span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain"></span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">      --rw</span><span class="token operator" style="color:#ff79c6">=</span><span class="token plain">randread </span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain"></span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">      --bs</span><span class="token operator" style="color:#ff79c6">=</span><span class="token plain">4K </span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain"></span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">      --numjobs</span><span class="token operator" style="color:#ff79c6">=</span><span class="token number" style="color:#50fa7b">8</span><span class="token plain"> </span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain"></span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">      --ioengine</span><span class="token operator" style="color:#ff79c6">=</span><span class="token plain">sync </span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain"></span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">      --group_reporting </span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain"></span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">      --runtime</span><span class="token operator" style="color:#ff79c6">=</span><span class="token number" style="color:#50fa7b">60</span><span class="token plain"> </span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain"></span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">/</span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain"></span><span class="token punctuation" style="color:#f8f8f2">..</span><span class="token plain">.</span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">Run status group </span><span class="token number" style="color:#50fa7b">0</span><span class="token plain"> </span><span class="token punctuation" style="color:#f8f8f2">(</span><span class="token plain">all </span><span class="token function" style="color:#8be9fd">jobs</span><span class="token punctuation" style="color:#f8f8f2">)</span><span class="token plain">:</span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">   READ: </span><span class="token assign-left variable" style="color:#ff79c6">bw</span><span class="token operator" style="color:#ff79c6">=</span><span class="token plain">223MiB/s </span><span class="token punctuation" style="color:#f8f8f2">(</span><span class="token plain">234MB/s</span><span class="token punctuation" style="color:#f8f8f2">)</span><span class="token plain">, 223MiB/s-223MiB/s </span><span class="token punctuation" style="color:#f8f8f2">(</span><span class="token plain">234MB/s-234MB/s</span><span class="token punctuation" style="color:#f8f8f2">)</span><span class="token plain">, </span><span class="token assign-left variable" style="color:#ff79c6">io</span><span class="token operator" style="color:#ff79c6">=</span><span class="token number" style="color:#50fa7b">13</span><span class="token plain">.1GiB </span><span class="token punctuation" style="color:#f8f8f2">(</span><span class="token number" style="color:#50fa7b">14</span><span class="token plain">.0GB</span><span class="token punctuation" style="color:#f8f8f2">)</span><span class="token plain">, </span><span class="token assign-left variable" style="color:#ff79c6">run</span><span class="token operator" style="color:#ff79c6">=</span><span class="token number" style="color:#50fa7b">60001</span><span class="token plain">-60001msec</span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">Disk stats </span><span class="token punctuation" style="color:#f8f8f2">(</span><span class="token plain">read/write</span><span class="token punctuation" style="color:#f8f8f2">)</span><span class="token plain">:</span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">  nvme0n1: </span><span class="token assign-left variable" style="color:#ff79c6">ios</span><span class="token operator" style="color:#ff79c6">=</span><span class="token number" style="color:#50fa7b">3166224</span><span class="token plain">/361, </span><span class="token assign-left variable" style="color:#ff79c6">merge</span><span class="token operator" style="color:#ff79c6">=</span><span class="token number" style="color:#50fa7b">0</span><span class="token plain">/318, </span><span class="token assign-left variable" style="color:#ff79c6">ticks</span><span class="token operator" style="color:#ff79c6">=</span><span class="token number" style="color:#50fa7b">217837</span><span class="token plain">/455, </span><span class="token assign-left variable" style="color:#ff79c6">in_queue</span><span class="token operator" style="color:#ff79c6">=</span><span class="token number" style="color:#50fa7b">218357</span><span class="token plain">, </span><span class="token assign-left variable" style="color:#ff79c6">util</span><span class="token operator" style="color:#ff79c6">=</span><span class="token number" style="color:#50fa7b">50.72</span><span class="token plain">%</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB">Copy</button></div></div><p>Here we&#x27;re using 8 threads to make blocking read calls to the same CSV file and
observe 223MB/s read rate which is not bad at all.</p><p>Now, we use io_uring to do the same job:</p><div class="codeBlockContainer_J+bg"><div class="codeBlockContent_csEI bash"><div tabindex="0" class="prism-code language-bash codeBlock_rtdJ thin-scrollbar"><div class="codeBlockLines_1zSZ" style="color:#f8f8f2;background-color:#262833"><div class="token-line" style="color:#f8f8f2"><span class="token plain">$ fio --name</span><span class="token operator" style="color:#ff79c6">=</span><span class="token plain">read_io_uring_4k </span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain"></span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">      --filename</span><span class="token operator" style="color:#ff79c6">=</span><span class="token plain">./hits.csv </span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain"></span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">      --rw</span><span class="token operator" style="color:#ff79c6">=</span><span class="token plain">randread </span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain"></span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">      --bs</span><span class="token operator" style="color:#ff79c6">=</span><span class="token plain">4K </span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain"></span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">      --numjobs</span><span class="token operator" style="color:#ff79c6">=</span><span class="token number" style="color:#50fa7b">8</span><span class="token plain"> </span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain"></span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">      --ioengine</span><span class="token operator" style="color:#ff79c6">=</span><span class="token plain">io_uring </span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain"></span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">      --iodepth</span><span class="token operator" style="color:#ff79c6">=</span><span class="token number" style="color:#50fa7b">64</span><span class="token plain"> </span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain"></span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">      --group_reporting </span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain"></span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">      --runtime</span><span class="token operator" style="color:#ff79c6">=</span><span class="token number" style="color:#50fa7b">60</span><span class="token plain"> </span><span class="token punctuation" style="color:#f8f8f2">\</span><span class="token plain"></span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">/</span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain"></span><span class="token punctuation" style="color:#f8f8f2">..</span><span class="token plain">.</span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">Run status group </span><span class="token number" style="color:#50fa7b">0</span><span class="token plain"> </span><span class="token punctuation" style="color:#f8f8f2">(</span><span class="token plain">all </span><span class="token function" style="color:#8be9fd">jobs</span><span class="token punctuation" style="color:#f8f8f2">)</span><span class="token plain">:</span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">   READ: </span><span class="token assign-left variable" style="color:#ff79c6">bw</span><span class="token operator" style="color:#ff79c6">=</span><span class="token plain">2232MiB/s </span><span class="token punctuation" style="color:#f8f8f2">(</span><span class="token plain">2340MB/s</span><span class="token punctuation" style="color:#f8f8f2">)</span><span class="token plain">, 2232MiB/s-2232MiB/s </span><span class="token punctuation" style="color:#f8f8f2">(</span><span class="token plain">2340MB/s-2340MB/s</span><span class="token punctuation" style="color:#f8f8f2">)</span><span class="token plain">, </span><span class="token assign-left variable" style="color:#ff79c6">io</span><span class="token operator" style="color:#ff79c6">=</span><span class="token plain">131GiB </span><span class="token punctuation" style="color:#f8f8f2">(</span><span class="token plain">140GB</span><span class="token punctuation" style="color:#f8f8f2">)</span><span class="token plain">, </span><span class="token assign-left variable" style="color:#ff79c6">run</span><span class="token operator" style="color:#ff79c6">=</span><span class="token number" style="color:#50fa7b">60003</span><span class="token plain">-60003msec</span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain" style="display:inline-block">
</span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">Disk stats </span><span class="token punctuation" style="color:#f8f8f2">(</span><span class="token plain">read/write</span><span class="token punctuation" style="color:#f8f8f2">)</span><span class="token plain">:</span></div><div class="token-line" style="color:#f8f8f2"><span class="token plain">  nvme0n1: </span><span class="token assign-left variable" style="color:#ff79c6">ios</span><span class="token operator" style="color:#ff79c6">=</span><span class="token number" style="color:#50fa7b">25482866</span><span class="token plain">/16240, </span><span class="token assign-left variable" style="color:#ff79c6">merge</span><span class="token operator" style="color:#ff79c6">=</span><span class="token number" style="color:#50fa7b">6262</span><span class="token plain">/571137, </span><span class="token assign-left variable" style="color:#ff79c6">ticks</span><span class="token operator" style="color:#ff79c6">=</span><span class="token number" style="color:#50fa7b">27625314</span><span class="token plain">/25206, </span><span class="token assign-left variable" style="color:#ff79c6">in_queue</span><span class="token operator" style="color:#ff79c6">=</span><span class="token number" style="color:#50fa7b">27650786</span><span class="token plain">, </span><span class="token assign-left variable" style="color:#ff79c6">util</span><span class="token operator" style="color:#ff79c6">=</span><span class="token number" style="color:#50fa7b">98.86</span><span class="token plain">%</span></div></div></div><button type="button" aria-label="Copy code to clipboard" class="copyButton_M3SB">Copy</button></div></div><p>We get an impressive 2,232MB/s this time. Also, it is worth noting that disk
utilization has increased to 98.86% against 50.72% in the previous fio run, all
of that with the same number of threads.</p><p>This simple experiment proved to us that io_uring may be a great fit in our
parallel <code>COPY</code> implementation, so we added an experimental API and continued
our experiments. As a result, QuestDB checks the kernel version and, if it&#x27;s new
enough, uses io_uring to speed up the import. Our code is also smart enough to
detect in-order adjacent lines and read these lines in one I/O operation. Thanks
to such behavior, parallel COPY is faster than the serial counterpart even on
ordered files.</p><p>We have explained why presence of a NVMe SSD made such a change in our
introductory benchmarks. EBS volumes are very convenient, but they show an order
of magnitude less IOPS and throughput rates than a physically attached drive.
Thus, using such drive for the purposes of initial data import makes a lot of
sense, especially when we consider a few terabytes to be imported.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_WiXH" id="whats-next"></a>What&#x27;s next?<a class="hash-link" href="#whats-next" title="Direct link to heading">#</a></h2><p>Prior to QuestDB 6.5, importing large amounts of unsorted data into a
partitioned table was practically impossible. We hope that our users will
appreciate this feature, as well as other improvements we&#x27;ve made recently. As a
logical next step, we want to take our data import one step further by making it
available and convenient to use in QuestDB Cloud. Finally, needless to say,
we&#x27;ll be thinking of more use cases for io_uring in our database.</p><p>As usual, we encourage you to try out the latest QuestDB 6.5.2 release and share
your feedback with our <a href="https://slack.questdb.io" target="_blank" rel="noopener noreferrer">Slack Community</a>. You can also
play with our <a href="https://demo.questdb.io" target="_blank" rel="noopener noreferrer">live demo</a> to see how fast it executes
your queries. And, of course, contributions to our open
source <a href="https://github.com/questdb/questdb" target="_blank" rel="noopener noreferrer">project on GitHub</a> are more than
welcome.</p></div><footer class="row margin-vert--lg"><div class="col"><strong>Tags:</strong><a class="margin-horiz--sm" href="/blog/tags/benchmark">benchmark</a><a class="margin-horiz--sm" href="/blog/tags/engineering">engineering</a><a class="margin-horiz--sm" href="/blog/tags/release">release</a><a class="margin-horiz--sm" href="/blog/tags/performance">performance</a><a class="margin-horiz--sm" href="/blog/tags/clickhouse">clickhouse</a></div></footer></article><div></div><div class="margin-vert--xl"><nav class="pagination-nav" aria-label="Blog post page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/blog/2022/09/30/hacktoberfest-questdb"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">« Join Hacktoberfest 2022 and contribute to QuestDB!</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/blog/2022/08/22/using-birch-anomaly-detection-questdb"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">Using BIRCH for anomaly detection with QuestDB »</div></a></div></nav></div></main></div></div><div class="root_0lLS"><div class="cards_xeZk"><div class="root_R798 skinPrimary_JHOH"><svg width="76" height="76" viewBox="0 0 76 76" fill="none" xmlns="http://www.w3.org/2000/svg"><circle cx="38" cy="38" r="38" fill="#1E1F27"></circle><path d="M56.99 18.792a.817.817 0 00-.117-.303c-.01-.02-.02-.05-.03-.069-.01-.02-.029-.029-.048-.049a.912.912 0 00-.137-.136l-.117-.088c-.02-.01-.03-.03-.05-.04-.029-.01-.068-.029-.107-.039a.85.85 0 00-.146-.049 1.075 1.075 0 00-.176-.019h-.147a1.15 1.15 0 00-.196.049c-.029.01-.068.01-.097.02L12.59 36.67a.979.979 0 00-.509 1.29c.098.216.264.401.48.5l16.07 7.556v15.025c0 .537.44.977.978.977a.964.964 0 00.831-.469l6.794-11.046 10.499 5.533a.974.974 0 001.408-.655l7.84-36.2v-.058c.01-.058.01-.107.019-.156 0-.059 0-.117-.01-.176zM29.413 44.208L15.376 37.6l36.326-15.68-22.289 22.288zm1.174 13.363V47.004l4.917 2.59-4.917 7.977zm16.92-3.87l-9.237-4.86-1.662-1.026-.068.118-5.289-2.786 23.1-23.12L47.508 53.7z" fill="#fff"></path></svg><h3 class="title_BHe0">문의하기</h3><p class="description_Bwvi">이메일을 입력하시면 소개자료를 보내드립니다.</p><div class="content_gAr2"><form class="root_teCd"><div><div class="inputs_0wQ8"><input type="email" class="input_cjQx input_a6pP" name="email" required="" pattern="^[a-zA-Z0-9.!#$%&amp;&#x27;*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)+$" placeholder="Email address" title="Email address should be valid"><button class="submit_poRs button_f7Ff button--tertiary_vtfp button--uppercase_ESEN" type="submit">보내기</button></div></div></form></div></div></div></div></div><footer class="root_JnvJ"><div class="content_-gfC center_S24y"><img alt="QuestDB logo" class="logo_JrwJ" src="/img/navbar/tp_brand.png" width="108" height="27"><div class="tagline_T1tH"><p>인프라관리부터 서비스운영까지 클라우드환경에 최적화된 클라우드 네이티브 플랫폼</p></div><div class="links_1SgP"></div></div><div class="border_PW0V"><div class="bottom_wPs3 center_S24y">Copyright © 2023 Gitple. All rights are reserved.</div></div></footer></div>
<script src="/assets/js/main.e53824dd.js" defer="defer"></script>
</body>
</html>