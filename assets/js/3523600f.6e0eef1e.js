"use strict";(self.webpackChunkquestdb_io=self.webpackChunkquestdb_io||[]).push([[1214],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return m}});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),c=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=c(n),m=o,h=d["".concat(l,".").concat(m)]||d[m]||u[m]||r;return n?a.createElement(h,i(i({ref:t},p),{},{components:n})):a.createElement(h,i({ref:t},p))}));function m(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,i=new Array(r);i[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,i[1]=s;for(var c=2;c<r;c++)i[c]=n[c];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},25033:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return l},metadata:function(){return c},toc:function(){return p},default:function(){return d}});var a=n(83117),o=n(80102),r=(n(67294),n(3905)),i=n(46092),s=["components"],l={title:"Building a Data Pipeline using QuestDB and Confluent Kafka",author:"Sooter Saalu",author_title:"Guest post",author_url:"https://github.com/soot3",author_image_url:"https://avatars.githubusercontent.com/soot3",description:"How to collate data with Kafka and implement a data pipeline that collects real-time ETH market data and stores data to QuestDB through Kafka connections.",keywords:["tutorial","python","ethereum","market data","crypto","kafka confluent"],image:"/img/blog/2022-06-07/banner.png",tags:["tutorial","python","kafka","cryptocurrency","eth","data science","market data"]},c={permalink:"/blog/2022/06/07/data-pipeline-with-kafka-and-questdb",source:"@site/blog/2022-06-07-data-pipeline-with-kafka-and-questdb.md",title:"Building a Data Pipeline using QuestDB and Confluent Kafka",description:"How to collate data with Kafka and implement a data pipeline that collects real-time ETH market data and stores data to QuestDB through Kafka connections.",date:"2022-06-07T00:00:00.000Z",formattedDate:"June 7, 2022",tags:[{label:"tutorial",permalink:"/blog/tags/tutorial"},{label:"python",permalink:"/blog/tags/python"},{label:"kafka",permalink:"/blog/tags/kafka"},{label:"cryptocurrency",permalink:"/blog/tags/cryptocurrency"},{label:"eth",permalink:"/blog/tags/eth"},{label:"data science",permalink:"/blog/tags/data-science"},{label:"market data",permalink:"/blog/tags/market-data"}],readingTime:9.665,truncated:!0,prevItem:{title:"Time Series Forecasting with TensorFlow and QuestDB",permalink:"/blog/2022/06/20/forecasting-with-questdb-and-tensorflow"},nextItem:{title:"QuestDB 6.4 Release Highlights",permalink:"/blog/2022/05/31/questdb-release-6-4"}},p=[{value:"Introduction",id:"introduction",children:[]},{value:"Data Pipelines Using Kafka and QuestDB",id:"data-pipelines-using-kafka-and-questdb",children:[]},{value:"Preparation",id:"preparation",children:[]},{value:"Install QuestDB and Kafka",id:"install-questdb-and-kafka",children:[]},{value:"Connect Kafka and QuestDB",id:"connect-kafka-and-questdb",children:[]},{value:"Generate market data using CoinCap API and Python",id:"generate-market-data-using-coincap-api-and-python",children:[]},{value:"Conclusion",id:"conclusion",children:[]}],u={toc:p};function d(e){var t=e.components,n=(0,o.Z)(e,s);return(0,r.kt)("wrapper",(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"This tutorial is a guest post contributed by\n",(0,r.kt)("a",{parentName:"em",href:"https://github.com/soot3"},"Sooter Saalu"),", who put together a tutorial to show\nyou how to build a data pipeline using Confluent Kafka, QuestDB, and Python. If\nyou like this content or have any feedback, feel free to reach out to the author\nor to us on ",(0,r.kt)("a",{parentName:"em",href:"https://github.com/questdb/questdb"},"GitHub")," or on\n",(0,r.kt)("a",{parentName:"em",href:"https://"},"Slack"),".")),(0,r.kt)(i.Z,{alt:"QuestDB 6.4 Release Highlights",height:800,src:"/img/blog/2022-06-07/banner.png",width:900,mdxType:"Banner"}),(0,r.kt)("h2",{id:"introduction"},"Introduction"),(0,r.kt)("p",null,"A data pipeline, at its base, is a series of data processing measures that is\nused to automate the transport and transformation of data between systems or\ndata stores. Data pipelines can be used for a wide range of use cases in a\nbusiness, including aggregating data on customers for recommendation purposes or\ncustomer relationship management, combining and transforming data from multiple\nsources, as well as collating/streaming real-time data from sensors or\ntransactions."),(0,r.kt)("p",null,"For example, a company like Airbnb could have data pipelines that go back and\nforth between their application and their platform of choice to improve customer\nservice. Netflix utilizes a recommendation data pipeline that automates the data\nscience steps for generating movie and series recommendations. Also, depending\non the rate at which it updates, a batch or streaming data pipeline can be used\nto generate and update the data used in an analytics dashboard for stakeholders."),(0,r.kt)("p",null,"In this article, you will learn how to implement a data pipeline that utilizes\nKafka to aggregate data from multiple sources into a QuestDB database.\nSpecifically, you will see the implementation of a streaming data pipeline that\ncollates cryptocurrency market price data from CoinCap into a QuestDB instance\nwhere metrics, analytics, and further dashboards can be made."),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://i.imgur.com/F0ysUIp.png",alt:"Rough architecture diagram"})),(0,r.kt)("h2",{id:"data-pipelines-using-kafka-and-questdb"},"Data Pipelines Using Kafka and QuestDB"),(0,r.kt)("p",null,"Data pipelines are made up of a data source (e.g., applications, databases, or\nweb services), a processing or transformation procedure (actions such as moving\nor modifying the data, occurring in parallel or in sequence with each other),\nand a destination (e.g., another application, repository, or web service)."),(0,r.kt)("p",null,"The type or format of data being moved or transformed, the size of the data, and\nthe rate at which it will be moved or transformed (batch or stream processing)\nare some other considerations you need to be aware of when building data\npipelines. A data pipeline that only needs to be triggered once a month will be\ndifferent from one made to handle real-time notifications from your application."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://kafka.apache.org/"},(0,r.kt)("em",{parentName:"a"},"Apache Kafka"))," is an open source distributed event\nplatform optimized for processing and modifying streaming data in real time. It\nis a fast and scalable option for creating high-performing, low-latency data\npipelines and building functionality for the data integration of high-volume\nstreaming data from multiple sources. Kafka is a fairly popular tool used by\nthousands of\n",(0,r.kt)("a",{parentName:"p",href:"https://kafka.apache.org/powered-by#:~:text=Today%2C%20Kafka%20is%20used%20by,strategies%20with%20event%20streaming%20architecture."},"companies"),"."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://questdb.io/"},(0,r.kt)("em",{parentName:"a"},"QuestDB"))," is a high-performance, open source SQL database\ndesigned to process time series data with ease and speed. It is a relational\ncolumn-oriented database with applications in areas such as IoT, sensor data and\nobservability, financial services, and machine learning. The database\u2019s\nfunctionality is written in Java with a supported REST API and support for the\n",(0,r.kt)("a",{parentName:"p",href:"https://www.postgresql.org/"},"PostgreSQL")," wire protocol and\n",(0,r.kt)("a",{parentName:"p",href:"https://www.influxdata.com/"},"InfluxDB")," line protocol, allowing for multiple\nways to ingest and query data in QuestDB."),(0,r.kt)("h2",{id:"preparation"},"Preparation"),(0,r.kt)("p",null,"QuestDB can be installed using\n",(0,r.kt)("a",{parentName:"p",href:"https://questdb.io/docs/get-started/docker"},"Docker"),",\n",(0,r.kt)("a",{parentName:"p",href:"https://questdb.io/docs/get-started/binaries"},"TAR files"),", or a package manager\nsuch as ",(0,r.kt)("a",{parentName:"p",href:"https://questdb.io/docs/get-started/homebrew"},"Homebrew"),". And\n",(0,r.kt)("a",{parentName:"p",href:"https://www.confluent.io/"},"Confluent")," offers a Kafka distribution, Confluent\nPlatform, with addons that ease your data pipeline process and can be\n",(0,r.kt)("a",{parentName:"p",href:"https://docs.confluent.io/platform/current/quickstart/ce-docker-quickstart.html"},"installed"),"\nusing Docker images or its downloaded TAR file."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Note: Confluent Platform is licensed separately from Apache Kafka. If you wish\nto use this setup in production environment, make sure to read through the\n",(0,r.kt)("a",{parentName:"p",href:"https://docs.confluent.io/platform/current/installation/license.html"},"Confluent Platform Licenses"),".")),(0,r.kt)("p",null,"As for market data, you can utilize the ",(0,r.kt)("a",{parentName:"p",href:"https://docs.coincap.io/"},"CoinCap API"),"\nto collate ETH market data. But there are also other\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/ColinEberhardt/awesome-public-streaming-datasets"},"streaming financial data resources"),"."),(0,r.kt)("p",null,"All files used in the article are available in this\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/Soot3/coincap_kafka_questdb"},"Github repository"),". You can\nclone the repository to work through the steps directly:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"git clone https://github.com/Soot3/coincap_kafka_questdb.git\n")),(0,r.kt)("h2",{id:"install-questdb-and-kafka"},"Install QuestDB and Kafka"),(0,r.kt)("p",null,"For the purpose of this article, you can install both using a\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/Soot3/coincap_kafka_questdb/blob/main/docker/docker-compose.yml"},"Docker Compose file"),"\nthat creates the required Docker containers for the Kafka and QuestDB pipeline:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'---\nversion: "3"\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:7.0.1\n    hostname: zookeeper\n    container_name: zookeeper\n    ports:\n      - "2181:2181"\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n      ZOOKEEPER_TICK_TIME: 2000\n  broker:\n    image: confluentinc/cp-server:7.0.1\n    hostname: broker\n    container_name: broker\n    depends_on:\n      - zookeeper\n    ports:\n      - "9092:9092"\n      - "9101:9101"\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092\n      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0\n      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1\n      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1\n      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1\n      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1\n      KAFKA_JMX_PORT: 9101\n      KAFKA_JMX_HOSTNAME: localhost\n      KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:8081\n      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:29092\n      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1\n      CONFLUENT_METRICS_ENABLE: "true"\n      CONFLUENT_SUPPORT_CUSTOMER_ID: "anonymous"\n\n  kafka-connect:\n    image: yitaekhwang/cp-kafka-connect-postgres:6.1.0\n    hostname: connect\n    container_name: connect\n    depends_on:\n      - broker\n      - zookeeper\n    ports:\n      - "8083:8083"\n    environment:\n      CONNECT_BOOTSTRAP_SERVERS: "broker:29092"\n      CONNECT_REST_ADVERTISED_HOST_NAME: connect\n      CONNECT_REST_PORT: 8083\n      CONNECT_GROUP_ID: compose-connect-group\n      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs\n      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1\n      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000\n      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets\n      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1\n      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status\n      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1\n      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter\n      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter\n\n  questdb:\n    image: questdb/questdb:latest\n    pull_policy: always\n    hostname: questdb\n    container_name: questdb\n    ports:\n      - "9000:9000"\n      - "8812:8812"\n\n')),(0,r.kt)("p",null,"In sequential order, this Docker Compose file installs Confluent-managed Kafka\ntools,\n",(0,r.kt)("a",{parentName:"p",href:"https://www.cloudkarafka.com/blog/cloudkarafka-what-is-zookeeper.html"},"Zookeeper"),",\nand\n",(0,r.kt)("a",{parentName:"p",href:"https://developer.confluent.io/learn-kafka/apache-kafka/brokers/"},"Kafka broker"),",\nwhich manage the connections and processes in the Kafka ecosystem. Then, it\ninstalls a\n",(0,r.kt)("a",{parentName:"p",href:"https://docs.confluent.io/kafka-connect-jdbc/current/index.html"},"JDBC Connector"),"\nthat will enable the connection between Kafka and any relational database such\nas QuestDB, this particular\n",(0,r.kt)("a",{parentName:"p",href:"https://hub.docker.com/r/yitaekhwang/cp-kafka-connect-postgres"},"JDBC connector image"),"\nis a custom connector created to simplify the connection between Confluent\u2019s\nKafka service and your Postgres database Finally, it installs the latest version\nof QuestDB."),(0,r.kt)("p",null,"You can set up Kafka and QuestDB by moving to the Docker directory and then\nrunning the Docker Compose file:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"cd  coincap_kafka_questdb/docker\ndocker-compose up -d\n")),(0,r.kt)("p",null,"The installation process should take a few minutes. You can check if the\nservices are up and working with ",(0,r.kt)("inlineCode",{parentName:"p"},"docker-compose ps"),". Once you see the ",(0,r.kt)("inlineCode",{parentName:"p"},"connect"),"\ncontainer status as ",(0,r.kt)("inlineCode",{parentName:"p"},"healthy")," your cluster will be ready to go."),(0,r.kt)("h2",{id:"connect-kafka-and-questdb"},"Connect Kafka and QuestDB"),(0,r.kt)("p",null,"At this point, your Kafka cluster and QuestDB instance are still unconnected,\nwith no avenue to pass data between them. Using your installed connector, you\ncan create this connection by setting the configuration settings for the\nconnector:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'{\n  "name": "postgres-sink-eth",\n  "config": {\n    "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",\n    "tasks.max": "1",\n    "topics": "topic_ETH",\n    "key.converter": "org.apache.kafka.connect.storage.StringConverter",\n    "value.converter": "org.apache.kafka.connect.json.JsonConverter",\n    "connection.url": "jdbc:postgresql://questdb:8812/qdb?useSSL=false",\n    "connection.user": "admin",\n    "connection.password": "quest",\n    "key.converter.schemas.enable": "false",\n    "value.converter.schemas.enable": "true",\n    "auto.create": "false",\n    "insert.mode": "insert",\n    "pk.mode": "none"\n  }\n}\n')),(0,r.kt)("p",null,"Here, you are setting the topic or topics that the connection monitors, the\nformat for the message entries, and the authentication details for the\nconnection. QuestDB then accepts ",(0,r.kt)("em",{parentName:"p"},"admin")," and ",(0,r.kt)("em",{parentName:"p"},"quest")," as user and password by\ndefault."),(0,r.kt)("p",null,"You can send this configuration to your installed connector using the following\ncommand:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},'curl -X POST -H "Accept:application/json" -H "Content-Type:application/json" --data @postgres-sink-eth.json http://localhost:8083/connectors\n')),(0,r.kt)("p",null,"When successfully executed, you should be able to see a response in JSON that\nincludes the configuration above."),(0,r.kt)("p",null,"At this point, you have a connected QuestDB instance that will monitor the\n",(0,r.kt)("inlineCode",{parentName:"p"},"topic_ETH")," topic and pull any records sent to it for storage on the database.\nYou can then create a table for the records on your database."),(0,r.kt)("p",null,"QuestDB has an interactive web console for that and can be accessed at\n",(0,r.kt)("inlineCode",{parentName:"p"},"http://localhost:9000/"),". Here, you can query the data and generate some simple\nvisualizations directly. Use the following command to create a table for your\nCoincap records"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"CREATE TABLE topic_ETH (`timestamp` timestamp, currency symbol, amount float)\n")),(0,r.kt)("p",null,"This creates a formatted table for your records, the next step involves\ngenerating records that will be sent to this table."),(0,r.kt)("h2",{id:"generate-market-data-using-coincap-api-and-python"},"Generate market data using CoinCap API and Python"),(0,r.kt)("p",null,"Using CoinCap\u2019s API and some Python code, you can create a Kafka Producer that\nwill generate data in real time:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},"# importing packages\nimport time, json\nimport datetime as dt\nimport requests\nfrom kafka import KafkaProducer\n\n# initializing Kafka Producer Client\nproducer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n                         value_serializer=lambda x:\n                         json.dumps(x,default=str).encode('utf-8'))\n\nprint('Initialized Kafka producer at {}'.format(dt.datetime.utcnow()))\n\n# Creating a continuous loop to process the real-time data\nwhile True:\n    # API request\n    uri = 'http://api.coincap.io/v2/assets/ethereum'\n    res = requests.request(\"GET\",uri)\n\n    start_time = time.time()\n    # Processing API response if successful\n    if (res.status_code==200):\n    # read json response\n        raw_data = json.loads(res.content)\n\n        # add the schema for Kafka\n        data = {'schema': {\n            'type': 'struct',\n            'fields': [{'type': 'string', 'optional': False, 'field': 'currency'\n                    }, {'type': 'float', 'optional': False, 'field': 'amount'\n                    }, {'type': 'string', 'optional': False,\n                    'field': 'timestamp'}],\n            'optional': False,\n            'name': 'Coincap',\n            }, 'payload': {'timestamp': dt.datetime.utcnow(),\n                        'currency': raw_data['data']['id'],\n                        'amount': float(raw_data['data']['priceUsd'])}}\n\n        print('API request succeeded at time {0}'.format(dt.datetime.utcnow()))\n\n        producer.send(topic=\"topic_ETH\",value=data)\n\n        print('Sent record to topic at time {}'.format(dt.datetime.utcnow()))\n\n    else:\n        print('Failed API request at time {0}'.format(dt.datetime.utcnow()))\n\n    end_time = time.time()\n    time_interval = end_time - start_time\n    # setting the API to be queried every 15 seconds\n    time.sleep(15 - time_interval)\n")),(0,r.kt)("p",null,"This Python code queries the CoinCap API in a continuous loop that generates the\nmarket price of ETH every 15 seconds. It then processes this data and sends it\nto the Kafka topic ",(0,r.kt)("inlineCode",{parentName:"p"},"topic_ETH"),", where it can be consumed by QuestDB. The data\nschema and payload used here is just an example as it doesn\u2019t utilize some\nQuestDB optimizations such as partitions"),(0,r.kt)("p",null,"You can run this code with the following commands:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"# move back to the parent directory\ncd ..\n# Installing needed Python packages\npip install -r requirements.txt\n# Run the python code\npython producer.py\n")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Note: If you are having issues installing the dependencies using the\n",(0,r.kt)("inlineCode",{parentName:"p"},"requirements.txt")," file, particularly if you are getting a\n",(0,r.kt)("inlineCode",{parentName:"p"},"Microsoft Visual C++")," error, please check your Python version first. The\nConfluent-Kafka-Python package supports a few\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/confluentinc/confluent-kafka-python/issues/805"},"Python versions"),"\non Windows at the time of writing, specifically Python 3.7, 3.8, and 3.9. If\nyou get an error that ",(0,r.kt)("inlineCode",{parentName:"p"},"librdkafka/rdkafka.h")," is not found, you can try\nfollowing the steps at this\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/confluentinc/confluent-kafka-python/issues/180"},"GitHub Issue"),".\nIn our particular case with an Apple M1 we solved this problem by executing"),(0,r.kt)("pre",{parentName:"blockquote"},(0,r.kt)("code",{parentName:"pre"},"brew install librdkafka\nexport LIBRARY_PATH=/opt/homebrew/Cellar/librdkafka/1.8.2/lib\nexport C_INCLUDE_PATH=/opt/homebrew/Cellar/librdkafka/1.8.2/include\npip3 install -r requirements.txt\n"))),(0,r.kt)("p",null,"With the producer script up and running, you will be collating ETH market prices\nevery fifteen seconds, where this data will be sent to your Kafka topic. Your\nQuestDB instance then automatically updates its database with the data from the\nmonitored Kafka topic. With this data pipeline and connection in place, your\nQuestDB instance will be populated with data at fifteen-seconds intervals."),(0,r.kt)("p",null,"Try running the following command a few times to observe the database updating\nitself from the Kafka topic:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},"SELECT * FROM 'topic_ETH'\n")),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://i.imgur.com/6yj1dU1.png",alt:"QuestDB result"})),(0,r.kt)("p",null,"With the data on your QuestDB instance, you can query or modify it and even\ngenerate more records to be sent to other Kafka topics, creating materialized\nviews and key performance indicators from your data."),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},"Note: To take down the installed containers used in this article, move to the\n",(0,r.kt)("inlineCode",{parentName:"p"},"coincap_kafka_questdb/docker")," directory and run the following command:")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-shell"},"docker-compose down\n")),(0,r.kt)("h2",{id:"conclusion"},"Conclusion"),(0,r.kt)("p",null,"Data pipelines are a central consideration in the effective movement and\ntransformation of data for your use. To efficiently collate raw data from where\nthey are generated and transform this raw data into valuable insights, you need\ndata pipelines."),(0,r.kt)("p",null,"In this article, you learned how to collate data with Kafka and implement a data\npipeline that collects real-time ETH market data and stores data to QuestDB\nthrough Kafka connections."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://github.com/questdb/questdb"},(0,r.kt)("em",{parentName:"a"},"QuestDB"))," is an open source SQL database\nwith a focus on fast performance and ease of use. It is an optimized storage for\nhigh-volume time series data, whether from your financial services or sensor\napplications, where time series data are constantly being generated. It\nsatisfies the need for high-performance ingestion and query times. Used in\nconjunction with Kafka, you can aggregate data from multiple sources, modify\nthem, and store them for use at a constantly-updating rate that fits your end\nuser or application."))}d.isMDXComponent=!0},86010:function(e,t,n){function a(e){var t,n,o="";if("string"==typeof e||"number"==typeof e)o+=e;else if("object"==typeof e)if(Array.isArray(e))for(t=0;t<e.length;t++)e[t]&&(n=a(e[t]))&&(o&&(o+=" "),o+=n);else for(t in e)e[t]&&(o&&(o+=" "),o+=t);return o}function o(){for(var e,t,n=0,o="";n<arguments.length;)(e=arguments[n++])&&(t=a(e))&&(o&&(o+=" "),o+=t);return o}n.d(t,{Z:function(){return o}})}}]);