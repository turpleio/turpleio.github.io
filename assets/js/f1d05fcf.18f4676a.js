"use strict";(self.webpackChunkquestdb_io=self.webpackChunkquestdb_io||[]).push([[6066],{3905:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return p}});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=a.createContext({}),c=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},u=function(e){var t=c(e.components);return a.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},h=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),h=c(n),p=o,m=h["".concat(s,".").concat(p)]||h[p]||d[p]||i;return n?a.createElement(m,r(r({ref:t},u),{},{components:n})):a.createElement(m,r({ref:t},u))}));function p(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,r=new Array(i);r[0]=h;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:o,r[1]=l;for(var c=2;c<i;c++)r[c]=n[c];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},12609:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return c},metadata:function(){return u},toc:function(){return d},default:function(){return p}});var a=n(83117),o=n(80102),i=(n(67294),n(3905)),r=n(46092),l=n(72525),s=["components"],c={title:"Automating ETL jobs on time series data with QuestDB on Google Cloud Platform",author:"G\xe1bor Boros",author_title:"Guest",author_url:"https://github.com/",author_image_url:"https://avatars.githubusercontent.com/gabor-boros",description:"Learn how to build an ETL job using Cloud Functions to extract data, remove personally-identifiable information, and load the transformed time series data into QuestDB.",keywords:["timeseries","etl","googlecloud"],tags:["tutorial","etl","googlecloud"],image:"/img/blog/shared/og-google-cloud-functions.png"},u={permalink:"/blog/2021/03/31/automating-etl-jobs-on-time-series-data-on-gcp",source:"@site/blog/2021-03-31-automating-etl-jobs-on-time-series-data-on-gcp.mdx",title:"Automating ETL jobs on time series data with QuestDB on Google Cloud Platform",description:"Learn how to build an ETL job using Cloud Functions to extract data, remove personally-identifiable information, and load the transformed time series data into QuestDB.",date:"2021-03-31T00:00:00.000Z",formattedDate:"March 31, 2021",tags:[{label:"tutorial",permalink:"/blog/tags/tutorial"},{label:"etl",permalink:"/blog/tags/etl"},{label:"googlecloud",permalink:"/blog/tags/googlecloud"}],readingTime:15.045,truncated:!1,prevItem:{title:"Streaming on-chain Ethereum data to QuestDB",permalink:"/blog/2021/04/12/stream-ethereum-data"},nextItem:{title:"Running QuestDB and Prometheus on GKE Autopilot",permalink:"/blog/2021/03/18/questdb-and-prometheus-on-gke-autopilot"}},d=[{value:"Introduction",id:"introduction",children:[]},{value:"What are ETL jobs for?",id:"what-are-etl-jobs-for",children:[]},{value:"Prerequisites",id:"prerequisites",children:[]},{value:"Creating an ETL job",id:"creating-an-etl-job",children:[{value:"Create a Compute Engine instance for QuestDB",id:"create-a-compute-engine-instance-for-questdb",children:[]},{value:"Allow networking on the instance",id:"allow-networking-on-the-instance",children:[]},{value:"Create a Storage bucket",id:"create-a-storage-bucket",children:[]},{value:"Create a Cloud Function",id:"create-a-cloud-function",children:[]}]},{value:"Generating and processing data",id:"generating-and-processing-data",children:[{value:"Inspecting the data",id:"inspecting-the-data",children:[]},{value:"Writing cloud functions",id:"writing-cloud-functions",children:[]}]},{value:"Running the full example",id:"running-the-full-example",children:[]},{value:"Summary",id:"summary",children:[]}],h={toc:d};function p(e){var t=e.components,n=(0,o.Z)(e,s);return(0,i.kt)("wrapper",(0,a.Z)({},h,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)(r.Z,{alt:"A display showing a dashboard with various analytics charts.",height:467,src:"/img/blog/2021-03-31/banner.jpg",width:650,mdxType:"Banner"},"Photo by ",(0,i.kt)("a",{href:"https://unsplash.com/photos/qwtCeJ5cLYs"},"Stephen Dawson")," ","via ",(0,i.kt)("a",{href:"https://unsplash.com"},"Unsplash")),(0,i.kt)("p",null,"This submission comes from one of our community contributors\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/gabor-boros"},"G\xe1bor Boros")," who has put together another\nexcellent tutorial showing how to use cloud functions together with QuestDB to\nbuild a custom ETL job that runs on time series data. The corresponding\nrepository for this tutorial with code examples is available to\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/gabor-boros/questdb-etl-jobs"},"browse on GitHub"),"."),(0,i.kt)("p",null,"Thanks for another great contribution, G\xe1bor!"),(0,i.kt)("h2",{id:"introduction"},"Introduction"),(0,i.kt)("p",null,"In the world of big data, software developers and data analysts often have to\nwrite scripts or complex software collections to process data before sending it\nto a data store for further analysis. This process is commonly called ETL, which\nstands for Extract, Transform and Load."),(0,i.kt)("h2",{id:"what-are-etl-jobs-for"},"What are ETL jobs for?"),(0,i.kt)("p",null,"Let's consider the following example: a medium-sized webshop with a few thousand\norders per day exports order information hourly. After a while, we would like to\nvisualize purchase trends, and we might want to share the results between\ndepartments or even publicly. Since the exported data contains personally\nidentifiable information (PII), we should anonymize it before using or exposing\nit to the public."),(0,i.kt)("p",null,"For the example above, we can use an ETL job to extract the incoming data,\nremove any PII and load the transformed data into a database used as the data\nvisualization backend later."),(0,i.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,i.kt)("p",null,"During this tutorial, we will use Python to write the cloud functions, so basic\npython knowledge is essential. Aside from these skills, you will need the\nfollowing resources:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"A ",(0,i.kt)("a",{parentName:"li",href:"https://console.cloud.google.com/getting-started"},"Google Cloud Platform"),"\n(GCP) account and a GCP Project."),(0,i.kt)("li",{parentName:"ul"},"Enable the\n",(0,i.kt)("a",{parentName:"li",href:"https://console.cloud.google.com/marketplace/product/google/cloudbuild.googleapis.com"},"Cloud Build API")," -\nwhen enabling APIs ",(0,i.kt)("strong",{parentName:"li"},"ensure that the correct GCP project is selected"),".")),(0,i.kt)("h2",{id:"creating-an-etl-job"},"Creating an ETL job"),(0,i.kt)("p",null,"As an intermediate data store where the webshop exports the data, we will use\nGoogle Storage and use Google Cloud Functions to transform it before loading it\ninto QuestDB."),(0,i.kt)(l.Z,{alt:"A diagram showing the flow of information and services used in this tutorial",height:118,src:"/img/blog/2021-03-31/workflow.png",width:650,mdxType:"Screenshot"}),(0,i.kt)("p",null,"We won't be building a webshop or a data exporter for an existing webshop, but\nwe will use a script to simulate the export to Google Storage. In the following\nsections, we will set up the necessary components on GCP. Ensure the required\nAPIs mentioned in the prerequisites are enabled, and that you have selected the\nGCP project in which you would like to create the tutorial resources."),(0,i.kt)("h3",{id:"create-a-compute-engine-instance-for-questdb"},"Create a Compute Engine instance for QuestDB"),(0,i.kt)("p",null,"First things first, we start with installing QuestDB on a virtual machine. To\nget started, navigate to the\n",(0,i.kt)("a",{parentName:"p",href:"https://console.cloud.google.com/compute/instances"},"Compute Engine console"),".\nVisiting this page for the first time will take a few moments to initialize.\nAfter the loading indicator has gone, start a new virtual machine:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Click on ",(0,i.kt)("strong",{parentName:"li"},"create")," and give the instance the name ",(0,i.kt)("inlineCode",{parentName:"li"},"questdb-vm")),(0,i.kt)("li",{parentName:"ol"},"Select a region that's geographically close to you"),(0,i.kt)("li",{parentName:"ol"},'Select the first generation "N1" series'),(0,i.kt)("li",{parentName:"ol"},"Choose the ",(0,i.kt)("inlineCode",{parentName:"li"},"f1-micro")," machine type - in a production environment you would\nchoose a more performant instance, but this is sufficient for example\npurposes"),(0,i.kt)("li",{parentName:"ol"},'In the "Container" section, check "Deploy a container image to this VM\ninstance", and enter ',(0,i.kt)("inlineCode",{parentName:"li"},"questdb/questdb:latest")),(0,i.kt)("li",{parentName:"ol"},'In the "Firewall" section, click on "Management, security, disks, networking,\nsole tenancy"'),(0,i.kt)("li",{parentName:"ol"},'In the new panel, select "Networking" and add ',(0,i.kt)("inlineCode",{parentName:"li"},"questdb"),' as a "Network tag"'),(0,i.kt)("li",{parentName:"ol"},'Leave all other settings with their defaults, and click on "create"')),(0,i.kt)("p",null,'Make sure you note the "External IP" of the instance as we will need that later.'),(0,i.kt)(l.Z,{alt:"The Google Cloud console UI showing QuestDB running on a Compute Engine instance",height:147,src:"/img/blog/2021-03-31/compute-engine.png",width:650,mdxType:"Screenshot"}),(0,i.kt)("p",null,"After a short time, the new instance will be up and running. As soon as the\ninstance is provisioned, we can initiate a remote session to install QuestDB by\nclicking ",(0,i.kt)("strong",{parentName:"p"},"ssh")," in the VM panel."),(0,i.kt)("h3",{id:"allow-networking-on-the-instance"},"Allow networking on the instance"),(0,i.kt)("p",null,"If we try to open the web console by opening the ",(0,i.kt)("inlineCode",{parentName:"p"},"http://<EXTERNAL_IP>:9000"),"\n(where ",(0,i.kt)("inlineCode",{parentName:"p"},"<EXTERNAL_IP>")," is the external IP of your virtual machine) it won't load\nand we will face a timeout. The reason behind this is that the firewall is not\nopened for port ",(0,i.kt)("inlineCode",{parentName:"p"},"9000")," yet."),(0,i.kt)("p",null,"To allow port ",(0,i.kt)("inlineCode",{parentName:"p"},"9000")," used by QuestDB, we must allow the port by adding a new\nfirewall rule on the\n",(0,i.kt)("a",{parentName:"p",href:"https://console.cloud.google.com/networking/firewalls/list"},"firewall console"),":"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},'Click on "create firewall rule" at the top of the page'),(0,i.kt)("li",{parentName:"ol"},'Give the rule the name "QuestDBPorts"'),(0,i.kt)("li",{parentName:"ol"},'In the "Target tags" field, write the same tag used for instance creation\n(',(0,i.kt)("inlineCode",{parentName:"li"},"questdb"),")"),(0,i.kt)("li",{parentName:"ol"},'For the "Source IP ranges" field, set ',(0,i.kt)("inlineCode",{parentName:"li"},"0.0.0.0/0")),(0,i.kt)("li",{parentName:"ol"},'In the "Protocols and ports" section, select ',(0,i.kt)("strong",{parentName:"li"},"tcp")," and set port to\n",(0,i.kt)("inlineCode",{parentName:"li"},"9000,8812")),(0,i.kt)("li",{parentName:"ol"},'Click on "create"')),(0,i.kt)("p",null,"Some seconds later, the rule will be applied on every instance with the matching\n",(0,i.kt)("inlineCode",{parentName:"p"},"questdb")," tag and port ",(0,i.kt)("inlineCode",{parentName:"p"},"9000")," will be open. You may ask what port ",(0,i.kt)("inlineCode",{parentName:"p"},"8812")," is for;\nthis port will be used by the Cloud Function later to connect to the database."),(0,i.kt)("p",null,"If you try to open the interactive console again, you should see the\n",(0,i.kt)("a",{parentName:"p",href:"/docs/develop/web-console"},"QuestDB Web Console")," and start writing queries.\nAs our first query, create the table in which the Cloud Function will write the\nanonymized data. To create the table run the following SQL statement:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-questdb-sql"},"CREATE TABLE\n    purchases(buyer STRING,\n            item_id INT,\n            quantity INT,\n            price INT,\n            purchase_date TIMESTAMP)\n    timestamp(purchase_date);\n")),(0,i.kt)("p",null,"The query above uses ",(0,i.kt)("inlineCode",{parentName:"p"},"timestamp(purchase_date)")," to set a designated timestamp on\nthe table so we can easily perform time series analysis in QuestDB. For more\ninformation on designated timestamps, see the official\n",(0,i.kt)("a",{parentName:"p",href:"/docs/concept/designated-timestamp"},"QuestDB documentation for timestamp"),"."),(0,i.kt)("h3",{id:"create-a-storage-bucket"},"Create a Storage bucket"),(0,i.kt)("p",null,"Now, we create the bucket where we will store the simulated webshop data.\nStorage buckets are in a single global namespace in GCP, which means that the\nbucket's name must be unique across all GCP customers. You can read more about\nStorage and buckets on Google's\n",(0,i.kt)("a",{parentName:"p",href:"https://cloud.google.com/storage/docs"},"documentation")," site."),(0,i.kt)("p",null,"To create a new bucket:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Navigate to the\n",(0,i.kt)("a",{parentName:"li",href:"https://console.cloud.google.com/storage"},"cloud storage console")),(0,i.kt)("li",{parentName:"ol"},"Select your project if not selected yet"),(0,i.kt)("li",{parentName:"ol"},'Click on "create bucket" and choose a unique name'),(0,i.kt)("li",{parentName:"ol"},"Select the same region as the instance above"),(0,i.kt)("li",{parentName:"ol"},'Leave other settings on default and click on "continue" to create the bucket')),(0,i.kt)("p",null,"If you successfully created the bucket, it should show up in the storage browser\nas you can see below."),(0,i.kt)(l.Z,{alt:"A new bucket storage resource on Google Cloud Platform",height:120,src:"/img/blog/2021-03-31/bucket-storage.png",width:650,mdxType:"Screenshot"}),(0,i.kt)("p",null,"At this point, we don't set any permissions, ACLs, or visibility settings on the\nbucket, but we will come back to that later."),(0,i.kt)("h3",{id:"create-a-cloud-function"},"Create a Cloud Function"),(0,i.kt)("p",null,"We have the bucket to upload the data, but we have nothing to process the data\nyet, and for this, we will use Cloud Functions to remove the PII."),(0,i.kt)("p",null,"Cloud Functions are functions as a service (FaaS) solution within GCP, similar\nto AWS Lambda. The functions are triggered by an event that can come from\nvarious sources. Our scenario Cloud Functions are convenient since we don't need\nto pay for a server to run all day, which is mostly idle; the function will be\nexecuted when the trigger event is fired, and we only pay for the execution time\nthe number of function calls."),(0,i.kt)("p",null,"To create a Cloud Function:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Navigate to\n",(0,i.kt)("a",{parentName:"li",href:"https://console.cloud.google.com/functions/list"},"cloud functions console")),(0,i.kt)("li",{parentName:"ol"},'Click on "create function" and give it the name ',(0,i.kt)("inlineCode",{parentName:"li"},"remove-pii")),(0,i.kt)("li",{parentName:"ol"},"Select the region we are using for other resources"),(0,i.kt)("li",{parentName:"ol"},'For "Trigger" select "Cloud Storage" from the dropdown list'),(0,i.kt)("li",{parentName:"ol"},'Set the event type to "Finalise/Create"'),(0,i.kt)("li",{parentName:"ol"},'Choose the bucket created above and click "variables, networking, and\nadvanced settings"'),(0,i.kt)("li",{parentName:"ol"},'Select "environment variables" on the tabbed panel'),(0,i.kt)("li",{parentName:"ol"},'Click on "add variable" right below the "Runtime environment variables"\nsection'),(0,i.kt)("li",{parentName:"ol"},"Add a new variable called ",(0,i.kt)("inlineCode",{parentName:"li"},"DATABASE_URL")," with the value\n",(0,i.kt)("inlineCode",{parentName:"li"},"postgresql://admin:quest@<EXTERNAL_IP>:8812/qdb"),", where ",(0,i.kt)("inlineCode",{parentName:"li"},"<EXTERNAL_IP>")," is\nthe external IP of your virtual machine"),(0,i.kt)("li",{parentName:"ol"},'Click "save" then "next"')),(0,i.kt)(l.Z,{alt:"Creating a new Cloud Function on Google Cloud Platform for an ETL job",height:586,src:"/img/blog/2021-03-31/create-cloud-function.png",width:450,mdxType:"Screenshot"}),(0,i.kt)("p",null,"The next step is to select the runtime our function will use and provide the\ncode. On this page, we can choose between numerous runtimes, including multiple\nversions of Python, Node.js, Go, Ruby, and even Java."),(0,i.kt)("p",null,"Since this tutorial uses Python, select ",(0,i.kt)("strong",{parentName:"p"},"Python 3.8"),' as it is the latest\nnon-beta version at the time of writing. Leave the rest of the settings as\ndefault, and write the function in the next section. Click "deploy" at the\nbottom of the page. Some seconds later, you will see that the deployment of the\nfunction is in progress.'),(0,i.kt)(l.Z,{alt:"Deploying a Python3.8 Cloud Function on Google Cloud Platform",height:150,src:"/img/blog/2021-03-31/deploying-cloud-function.png",width:550,mdxType:"Screenshot"}),(0,i.kt)("h2",{id:"generating-and-processing-data"},"Generating and processing data"),(0,i.kt)("p",null,"Before moving on, here's a quick recap on what we did so far:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Set up a new Google Storage bucket"),(0,i.kt)("li",{parentName:"ul"},"Created the Cloud Function which will process the data later on"),(0,i.kt)("li",{parentName:"ul"},"Connected the bucket with the function to trigger on a new object is created\non the bucket")),(0,i.kt)("p",null,"Now for the fun part of the tutorial: writing the data processing script and\nloading the data in the database. Let's write the function to remove PII, but\nfirst, talk a bit about the data's structure."),(0,i.kt)("h3",{id:"inspecting-the-data"},"Inspecting the data"),(0,i.kt)("p",null,"ETL jobs, by their nature, heavily depend on the structure of incoming data. A\njob may process multiple data sources, and data structure can vary per source.\nThe data structure we will use is simple, we have a CSV file with the following\ncolumns:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"customer email address"),(0,i.kt)("li",{parentName:"ul"},"purchase date"),(0,i.kt)("li",{parentName:"ul"},"item ID for the purchased item"),(0,i.kt)("li",{parentName:"ul"},"quantity"),(0,i.kt)("li",{parentName:"ul"},"the price per item")),(0,i.kt)("p",null,"As you see, there is no currency column since we will assume every price is in\none currency. To generate random data, you can use the\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/gabor-boros/questdb-etl-jobs/blob/e47b5f3191c8648f486cea207b317c92899c3bd1/data_generator.py"},"provided script"),"\nwritten for this tutorial."),(0,i.kt)("h3",{id:"writing-cloud-functions"},"Writing cloud functions"),(0,i.kt)("p",null,'By now, we have everything to write the data transformer function and test how\nwell the PII removal performs. We will work in the "inline editor" of the cloud\nfunction, so as a first step, open the edit the cloud function created above by\nnavigating to the\n',(0,i.kt)("a",{parentName:"p",href:"https://console.cloud.google.com/functions/list"},"cloud functions console"),' and\nclicking on the function\'s name. That will open the details of the function. At\nthe top, click on "edit", then at the bottom, click on "next" to open the\neditor.'),(0,i.kt)("p",null,"Let's start with the requirements. On the left-hand side, click on the\n",(0,i.kt)("inlineCode",{parentName:"p"},"requirements.txt")," and paste the following:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-txt"},"google-cloud-storage==1.36.2\npsycopg2==2.8.6\nsqlalchemy==1.4.2\n")),(0,i.kt)("p",null,"Here we add the required packages to connect to Google Storage and QuestDB.\nNext, click on ",(0,i.kt)("inlineCode",{parentName:"p"},"main.py"),", remove its whole content and start adding the\nfollowing:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import csv\nimport hashlib\nimport json\nimport logging\nimport os\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import List\n\nfrom google.cloud import storage\nfrom sqlalchemy.sql import text\nfrom sqlalchemy.engine import Connection, create_engine\n\nlogger = logging.getLogger(__name__)\n\n# Create a database engine\nengine = create_engine(os.getenv("DATABASE_URL"))\n')),(0,i.kt)("p",null,"As you may expect, we start with the imports, but we added two extra lines: one\nfor the logger and one for configuring the database engine. We will need the\nlogger to log warnings and exceptions during the execution, while we will use\nthe engine later to insert anonymized data into the database."),(0,i.kt)("p",null,"To make our job easier, we are going to add a data class, called ",(0,i.kt)("inlineCode",{parentName:"p"},"Record"),". This\ndata class will be used to store the parsed and anonymized CSV data for a line\nof the uploaded file."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},"# ...\n\n@dataclass\nclass Record:\n    buyer: str\n    item_id: int\n    quantity: int\n    price: int\n    purchase_date: datetime\n\n# ...\n")),(0,i.kt)("p",null,"As we discussed, ETL jobs are validating the data that they receive as input. In\nour case, we will trigger the function if an object is created on the storage.\nThis means any object, like a CSV, PDF, TXT, PNG file, or even a directory, is\ncreated, though we only want to execute CSV files' transformation. To validate\nthe incoming data, we write two simple validator functions:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# ...\n\ndef is_event_valid(event: dict) -> bool:\n    """\n    Validate that the event has all the necessary attributes required for the\n    execution.\n    """\n\n    attributes = event.keys()\n    required_parameters = ["bucket", "contentType", "name", "size"]\n\n    return all(parameter in attributes for parameter in required_parameters)\n\n\ndef is_object_valid(event: dict) -> bool:\n    """\n    Validate that the finalized/created object is a CSV file and its size is\n    greater than zero.\n    """\n\n    has_content = int(event["size"]) > 0\n    is_csv = event["contentType"] == "text/csv"\n\n    return has_content and is_csv\n\n# ...\n')),(0,i.kt)("p",null,"The first function will validate that event has all the necessary parameters,\nwhile the second function checks that the object created and triggered the event\nis a CSV and has any content. The next function we create is used to get an\nobject from the storage which, in our case, the file triggered the event:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# ...\n\ndef get_content(bucket: storage.Bucket, file_path: str) -> str:\n    """\n    Get the blob from the bucket and return its content as a string.\n    """\n\n    blob = bucket.get_blob(file_path)\n    return blob.download_as_string().decode("utf-8")\n\n# ...\n')),(0,i.kt)("p",null,"Anonymizing the data, in this scenario, is relatively easy, though we need to\nensure we can build statistics and visualizations later based on this data, so\nthe anonymized parts should be consistent for a user. To achieve this, we will\nhash the buyer's email address, so nobody may track it back to the person owning\nthe email, but we can use it for visualization:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# ...\n\ndef anonymize_pii(row: List[str]) -> Record:\n    """\n    Unpack and anonymize data.\n    """\n\n    email, item_id, quantity, price, purchase_date = row\n\n    # Anonymize email address\n    hashed_email = hashlib.sha1(email.encode()).hexdigest()\n\n    return Record(\n        buyer=hashed_email,\n        item_id=int(item_id),\n        quantity=int(quantity),\n        price=int(price),\n        purchase_date=purchase_date,\n    )\n\n# ...\n')),(0,i.kt)("p",null,"So far, we have functions to validate the data, get the file's content which\ntriggered the Cloud Function, and anonymize the data. The next thing we need to\nbe able to do is to load the data into our database. Up to this point, every\nfunction we have wrote was simple, and this one is no exception:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# ...\n\ndef write_to_db(conn: Connection, record: Record):\n    """\n    Write the records into the database.\n    """\n\n    query = """\n    INSERT INTO purchases(buyer, item_id, quantity, price, purchase_date)\n    VALUES(:buyer, :item_id, :quantity, :price, to_timestamp(:purchase_date, \'yyyy-MM-ddTHH:mm:ss\'));\n    """\n\n    try:\n        conn.execute(text(query), **record.__dict__)\n    except Exception as exc:\n        # If an error occures, log the exception and continue\n        logger.exception("cannot write record", exc_info=exc)\n\n# ...\n')),(0,i.kt)("p",null,"As you see, writing to the database is easy. We get the connection and the\nrecord we need to write into the database, prepare the query and execute it. In\ncase of an exception, we don't want to block the whole processing, so we catch\nthe exception, log it and let the script go on. If an exception occurred, we can\ncheck it later and fix the script or load the data manually."),(0,i.kt)("p",null,"The last bit is the glue code, which brings together these functions. Let's have\na look at that:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'# ...\n\ndef entrypoint(event: dict, context):\n    """\n    Triggered by a creation on a Cloud Storage bucket.\n    """\n\n    # Check if the event has all the necessary parameters. In case any of the\n    # required parameters are missing, return early not to waste execution time.\n    if not is_event_valid(event):\n        logger.error("invalid event: %s", json.dumps(event))\n        return\n\n    file_path = event["name"]\n\n    # Check if the created object is valid or not. In case the object is invalid\n    # return early not to waste execution time.\n    if not is_object_valid(event):\n        logger.warning("invalid object: %s", file_path)\n        return\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(event["bucket"])\n\n    data = get_content(bucket, file_path)\n    reader = csv.reader(data.splitlines())\n\n    # Anonymize PII and filter out invalid records\n    records: List[Record] = filter(lambda r: r, [anonymize_pii(row) for row in reader])\n\n    # Write the anonymized data to database\n    with engine.connect() as conn:\n        for record in records:\n            write_to_db(conn, record)\n')),(0,i.kt)("p",null,"In the example above, we call the two validators to ensure it worth processing\nthe data, and we get the file path from the event. After that we initialize the\nclient used to connect to Google Storage, then we get the object's content,\nparse the CSV file, and anonymize the content of it."),(0,i.kt)("p",null,"Last but not least, we connect to the database - defined by the ",(0,i.kt)("inlineCode",{parentName:"p"},"DATABASE_URL"),"\nconfigured for the engine and write all records to the database one by one."),(0,i.kt)("p",null,'As you see, the entrypoint of the function has been changed as well. In the text\nbox called "Entrypoint" set the entrypoint as a function name to call. The\nentrypoint is the function that will be called by Cloud Functions when an event\nis triggered.'),(0,i.kt)("h2",{id:"running-the-full-example"},"Running the full example"),(0,i.kt)("p",null,"We are close to finishing this tutorial, so it's time to test our Cloud\nFunction."),(0,i.kt)("p",null,"To test the Cloud Function:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Download the\n",(0,i.kt)("a",{parentName:"li",href:"https://github.com/gabor-boros/questdb-etl-jobs/blob/e47b5f3191c8648f486cea207b317c92899c3bd1/data_generator.py"},"pre-made script"),"\nand run it to generate random data."),(0,i.kt)("li",{parentName:"ol"},"Navigate to the bucket you created"),(0,i.kt)("li",{parentName:"ol"},'Above the list of objects in the bucket (which should be empty) click on\n"upload files"'),(0,i.kt)("li",{parentName:"ol"},"Select and upload the randomly-generated data"),(0,i.kt)("li",{parentName:"ol"},"After the file is uploaded, visit the\n",(0,i.kt)("a",{parentName:"li",href:"https://console.cloud.google.com/functions/list"},"Cloud Functions console")),(0,i.kt)("li",{parentName:"ol"},'Click on the actions button and select "view logs"'),(0,i.kt)("li",{parentName:"ol"},"Navigate to ",(0,i.kt)("inlineCode",{parentName:"li"},"http://<EXTERNAL_IP>:9000"),", where ",(0,i.kt)("inlineCode",{parentName:"li"},"<EXTERNAL_IP>")," is the\nexternal IP of your virtual machine")),(0,i.kt)("p",null,"We can now execute the following SQL query:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-questdb-sql"},"SELECT * FROM purchases ORDER BY purchase_date;\n")),(0,i.kt)("p",null,"As you can see, the data is loaded and we have no PII there. By creating a\nsimple chart, we can even observe trends in the generated data, how our\nimaginary buyers purchased items on the webshop."),(0,i.kt)(l.Z,{alt:"Visualizing SQL query results in the QuestDB Web Console",height:320,src:"/img/blog/2021-03-31/visualizing-data.png",width:650,mdxType:"Screenshot"}),(0,i.kt)("h2",{id:"summary"},"Summary"),(0,i.kt)("p",null,"We've installed QuestDB on Google Cloud Platform, set up a Google Storage bucket\nto store the simulated purchase data exports, built an ETL job that anonymized\nour buyers' data, and loaded it into a time series database, QuestDB. Data\nanalysts could write more jobs as Cloud Functions in multiple languages and set\nup multiple sources. It's also possible to set QuestDB's HTTP endpoints to\nread-only which would allow us to share this dashboard without worrying about\ndestructive commands."),(0,i.kt)("p",null,"If you like this content, we'd love to know your thoughts! Feel free to share\nyour feedback, browse the\n",(0,i.kt)("a",{parentName:"p",href:"https://github.com/gabor-boros/questdb-etl-jobs"},"GitHub repository for this tutorial"),",\nor come and say hello in the ",(0,i.kt)("a",{parentName:"p",href:"https://"},"QuestDB Community Slack"),"."))}p.isMDXComponent=!0},86010:function(e,t,n){function a(e){var t,n,o="";if("string"==typeof e||"number"==typeof e)o+=e;else if("object"==typeof e)if(Array.isArray(e))for(t=0;t<e.length;t++)e[t]&&(n=a(e[t]))&&(o&&(o+=" "),o+=n);else for(t in e)e[t]&&(o&&(o+=" "),o+=t);return o}function o(){for(var e,t,n=0,o="";n<arguments.length;)(e=arguments[n++])&&(t=a(e))&&(o&&(o+=" "),o+=t);return o}n.d(t,{Z:function(){return o}})}}]);