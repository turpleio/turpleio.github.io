"use strict";(self.webpackChunkquestdb_io=self.webpackChunkquestdb_io||[]).push([[653],{3905:function(e,t,n){n.d(t,{Zo:function(){return d},kt:function(){return u}});var r=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var c=r.createContext({}),l=function(e){var t=r.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},d=function(e){var t=l(e.components);return r.createElement(c.Provider,{value:t},e.children)},f={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},p=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,c=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),p=l(n),u=a,m=p["".concat(c,".").concat(u)]||p[u]||f[u]||o;return n?r.createElement(m,i(i({ref:t},d),{},{components:n})):r.createElement(m,i({ref:t},d))}));function u(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=p;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s.mdxType="string"==typeof e?e:a,i[1]=s;for(var l=2;l<o;l++)i[l]=n[l];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}p.displayName="MDXCreateElement"},7527:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return s},metadata:function(){return c},toc:function(){return l},default:function(){return f}});var r=n(83117),a=n(80102),o=(n(67294),n(3905)),i=["components"],s={title:"Ingestion from Kafka Overview",sidebar_label:"Overview",description:"Apache Kafka integration overview."},c={unversionedId:"third-party-tools/kafka/overview",id:"third-party-tools/kafka/overview",isDocsHomePage:!1,title:"Ingestion from Kafka Overview",description:"Apache Kafka integration overview.",source:"@site/docs/third-party-tools/kafka/overview.md",sourceDirName:"third-party-tools/kafka",slug:"/third-party-tools/kafka/overview",permalink:"/docs/third-party-tools/kafka/overview",editUrl:"https://github.com/turpleio/homepage/edit/main/docs/third-party-tools/kafka/overview.md",version:"current",sidebar_label:"Overview",frontMatter:{title:"Ingestion from Kafka Overview",sidebar_label:"Overview",description:"Apache Kafka integration overview."},sidebar:"docs",previous:{title:"Grafana",permalink:"/docs/third-party-tools/grafana"},next:{title:"QuestDB Kafka connector",permalink:"/docs/third-party-tools/kafka/questdb-kafka"}},l=[{value:"QuestDB Kafka connector",id:"questdb-kafka-connector",children:[]},{value:"JDBC connector",id:"jdbc-connector",children:[]},{value:"Dedicated program",id:"dedicated-program",children:[]},{value:"Stream processing engine",id:"stream-processing-engine",children:[]}],d={toc:l};function f(e){var t=e.components,n=(0,a.Z)(e,i);return(0,o.kt)("wrapper",(0,r.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"Ingesting data from Apache Kafka to QuestDB is a common use case. Possible\nstrategies are as the following:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"/docs/third-party-tools/kafka/questdb-kafka/"},"QuestDB Kafka connector"),": The\nrecommended strategy for connecting to Kafka using ILP and Kafka Connect."),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"/docs/third-party-tools/kafka/jdbc"},"JDBC connector"),": A generic connector\nusing Kafka Connect."),(0,o.kt)("li",{parentName:"ol"},"Write a dedicated program to read data from Kafka and write to QuestDB."),(0,o.kt)("li",{parentName:"ol"},"Use a stream processing engine.")),(0,o.kt)("p",null,"Each strategy has different trade-offs. The rest of this page discusses each\nstrategy and aims to guide advanced users."),(0,o.kt)("h2",{id:"questdb-kafka-connector"},"QuestDB Kafka connector"),(0,o.kt)("p",null,"QuestDB has developed a QuestDB Kafka connector for Kafka. The connector is\nbuilt on top of the Kafka Connect framework and uses the\n",(0,o.kt)("a",{parentName:"p",href:"/docs/develop/insert-data/#influxdb-line-protocol/"},"Influx Line Protocol (ILP)"),"\nfor communication with QuestDB. Kafka Connect handles concerns such as fault\ntolerance and serialization. It also provides facilities for message\ntransformations, filtering, etc. ILP ensures operational simplicity and\nexcellent performance: it can insert 100,000s rows per second."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"This is the recommended strategy for most users.")),(0,o.kt)("h2",{id:"jdbc-connector"},"JDBC connector"),(0,o.kt)("p",null,"Similar to the QuestDB Kafka connector, the JDBC connector also uses the Kafka\nConnect framework. However, instead of using a dedicated ILP, it relies on a\n",(0,o.kt)("a",{parentName:"p",href:"/docs/third-party-tools/kafka/jdbc/"},"generic JDBC binary")," and QuestDB\n",(0,o.kt)("a",{parentName:"p",href:"/docs/develop/connect/#postgresql-wire-protocol"},"PostgreSQL protocol compatibility"),".\nIt requires objects in Kafka to have associated schema and overall it is more\ncomplex to set up and run. Compared to the QuestDB Kafka connector, the JDBC\nconnector has significantly lower performance, but the following advantages:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"JDBC insertion allows higher consistency guarantees than the fire-and-forget\nILP method used by the QuestDB Kafka connector."),(0,o.kt)("li",{parentName:"ul"},"Various Kafka-as-a-Service providers often have the JDBC connector\npre-packaged.")),(0,o.kt)("p",null,"This strategy is recommended when the QuestDB Kafka connector cannot be used for\nsome reason."),(0,o.kt)("h2",{id:"dedicated-program"},"Dedicated program"),(0,o.kt)("p",null,"Writing a dedicated program reading from Kafka topics and writing to QuestDB\ntables offers great flexibility: The program can do arbitrary data\ntransformations and filtering, including stateful operations. On the other hand:\nIt's the most complex strategy to implement. You'll have to deal with different\nserialization formats, handle failures, etc. This strategy is recommended for\nvery advanced use cases only. This is not recommended for most users."),(0,o.kt)("h2",{id:"stream-processing-engine"},"Stream processing engine"),(0,o.kt)("p",null,"Stream processing engine provides a middle ground between writing a dedicated\nprogram and using one of the connectors. Engines such as\n",(0,o.kt)("a",{parentName:"p",href:"https://flink.apache.org/"},"Apache Flink")," provide rich API for data\ntransformations, enrichment, and filtering; at the same time, they can help you\nwith shared concerns such as fault-tolerance and serialization. However, they\noften have a non-trivial learning curve. QuestDB offers a\n",(0,o.kt)("a",{parentName:"p",href:"https://github.com/questdb/flink-questdb-connector"},"connector for Apache Flink"),".\nIt is the recommended strategy if you are an existing Flink user, and you need\nto do complex transformations while inserting entries from Kafka to QuestDB."))}f.isMDXComponent=!0}}]);