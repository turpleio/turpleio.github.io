"use strict";(self.webpackChunkquestdb_io=self.webpackChunkquestdb_io||[]).push([[638],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return m}});var a=n(67294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),h=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},p=function(e){var t=h(e.components);return a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),d=h(n),m=i,c=d["".concat(l,".").concat(m)]||d[m]||u[m]||r;return n?a.createElement(c,o(o({ref:t},p),{},{components:n})):a.createElement(c,o({ref:t},p))}));function m(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,o=new Array(r);o[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,o[1]=s;for(var h=2;h<r;h++)o[h]=n[h];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},71789:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return l},metadata:function(){return h},toc:function(){return p},default:function(){return d}});var a=n(83117),i=n(80102),r=(n(67294),n(3905)),o=n(72525),s=["components"],l={title:"Importing 300k rows/sec with io_uring",author:"Andrey Pechkurov",author_title:"QuestDB Engineering",author_url:"https://github.com/puzpuzpuz",author_image_url:"https://avatars.githubusercontent.com/puzpuzpuz",description:"QuestDB 6.5 introduces a new `COPY` commands allowing importing large CSV files. This article reveals the story behind it and highlights the exciting benchmark results using this new SQL command.",keywords:["io_uring","benchmark","questdb","time series"],tags:["benchmark","engineering","release","performance","clickhouse"],image:"/img/blog/2022-09-12/cover.png"},h={permalink:"/blog/2022/09/12/importing-300k-rows-with-io-uring",source:"@site/blog/2022-09-12-importing-300k-rows-with-io-uring.md",title:"Importing 300k rows/sec with io_uring",description:"QuestDB 6.5 introduces a new `COPY` commands allowing importing large CSV files. This article reveals the story behind it and highlights the exciting benchmark results using this new SQL command.",date:"2022-09-12T00:00:00.000Z",formattedDate:"September 12, 2022",tags:[{label:"benchmark",permalink:"/blog/tags/benchmark"},{label:"engineering",permalink:"/blog/tags/engineering"},{label:"release",permalink:"/blog/tags/release"},{label:"performance",permalink:"/blog/tags/performance"},{label:"clickhouse",permalink:"/blog/tags/clickhouse"}],readingTime:12.745,truncated:!0,prevItem:{title:"Join Hacktoberfest 2022 and contribute to QuestDB!",permalink:"/blog/2022/09/30/hacktoberfest-questdb"},nextItem:{title:"Using BIRCH for anomaly detection with QuestDB",permalink:"/blog/2022/08/22/using-birch-anomaly-detection-questdb"}},p=[{value:"Introduction",id:"introduction",children:[]},{value:"How ClickBench helped us improve",id:"how-clickbench-helped-us-improve",children:[]},{value:"The import speed-up",id:"the-import-speed-up",children:[]},{value:"Optimizing the import",id:"optimizing-the-import",children:[]},{value:"io_uring everything!",id:"io_uring-everything",children:[]},{value:"What&#39;s next?",id:"whats-next",children:[]}],u={toc:p};function d(e){var t=e.components,n=(0,i.Z)(e,s);return(0,r.kt)("wrapper",(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"In this blog post, QuestDB\u2019s very own\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/puzpuzpuz"},"Andrei Pechkurov")," presents how to ingest large\nCSV files a lot more efficiently using the SQL\n",(0,r.kt)("a",{parentName:"p",href:"https://questdb.io/docs/reference/sql/copy"},(0,r.kt)("inlineCode",{parentName:"a"},"COPY"))," statement, and takes us\nthrough the journey of benchmarking. Andrei also shares insights about how the\nnew improvement is made possible by\xa0",(0,r.kt)("inlineCode",{parentName:"p"},"io_uring")," and compares QuestDB's import\nversus several well-known OLAP and time-series databases in Clickhouse's\nClickBench benchmark."),(0,r.kt)("h2",{id:"introduction"},"Introduction"),(0,r.kt)("p",null,"As an open source time series database company, we understand that getting your\nexisting data into the database in a fast and convenient manner is as important\nas being able to ",(0,r.kt)("a",{parentName:"p",href:"https://questdb.io/time-series-benchmark-suite"},"ingest")," and\n",(0,r.kt)("a",{parentName:"p",href:"https://questdb.io/blog/2022/05/26/query-benchmark-questdb-versus-clickhouse-timescale"},"query"),"\nyour data efficiently later on. That's why we decided to dedicate our new\nrelease, QuestDB 6.5, to the new parallel\n",(0,r.kt)("a",{parentName:"p",href:"https://questdb.io/docs/guides/importing-data"},"CSV file import")," feature. In\nthis blog post, we discuss what parallel import means for our users and how it's\nimplemented internally. As a bonus, we also share how recent ClickHouse team's\nbenchmark helped us to improve both QuestDB and its demonstrated results."),(0,r.kt)("h2",{id:"how-clickbench-helped-us-improve"},"How ClickBench helped us improve"),(0,r.kt)("p",null,"Recently ClickHouse conducted a\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/ClickHouse/ClickBench"},"benchmark")," for their own database and\nmany others, including QuestDB. The benchmark included data import as the first\nstep. Since we were in the process of building a faster import, this benchmark\nprovided us with nice test data and baseline results. So, what have we achieved?\nLet's find out. The benchmark was using QuestDB's HTTP\n",(0,r.kt)("a",{parentName:"p",href:"https://questdb.io/docs/reference/api/rest#imp---import-data"},"import endpoint"),"\nto ingest the data into an existing non-partitioned table. You may wonder why it\ndoesn't use a ",(0,r.kt)("a",{parentName:"p",href:"https://questdb.io/docs/concept/partitions"},"partitioned")," table,\nwhich stores the data sorted by the timestamp values and provides many benefits\nfor time series analysis. Most likely, the reason is terrible import execution\ntime. Both HTTP-based import and pre-6.5 COPY SQL command are simply not capable\nof importing a big CSV file with unsorted data. Thus, the benchmark opts for a\nnon-partitioned table with no designated timestamp column. The test CSV file may\nbe downloaded and uncompressed following the commands:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"wget 'https://datasets.clickhouse.com/hits_compatible/hits.csv.gz'\ngzip -d hits.csv.gz\n")),(0,r.kt)("p",null,"The file is on the bigger side, 76GB when decompressed, and contains rows that\nare heavily out-of-order in terms of time. This makes it a nice import\nperformance challenge for any time series database. Getting the data into a\nlocally running QuestDB instance via HTTP is as simple as:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"curl -F data=@hits.csv 'http://localhost:9000/imp?name=hits'\n")),(0,r.kt)("p",null,"Such import took almost 28 minutes (1,668 seconds, to be precise) on a\nc6a.4xlarge EC2 instance with a 500GB gp2 volume in ClickBench. This yields\naround 47MB/s and leaves a lot to wish for. In contrast, it took ClickHouse\ndatabase around 8 minutes (476 seconds) to import the file on the same hardware.\nBut since we were already working on faster imports for partitioned tables, this\nbenchmark provided us with nice test data and baseline results."),(0,r.kt)("p",null,"In addition to import speed, ClickBench measures query performance. Although\nnone of the queries it ran were related to time series analysis, the results\nhelped us to improve QuestDB. We found and fixed a stability issue, as well as\nadded support for some SQL functions. Other than that, our SQL engine had a bug\naround multi-threaded ",(0,r.kt)("inlineCode",{parentName:"p"},"min()"),"/",(0,r.kt)("inlineCode",{parentName:"p"},"max()")," SQL function optimization: it was\ncase-sensitive and simply ignored ",(0,r.kt)("inlineCode",{parentName:"p"},"MIN()"),"/",(0,r.kt)("inlineCode",{parentName:"p"},"MAX()")," used in ClickBench. After a\ntrivial fix, queries using these aggregate functions got their intended speed\nback. Finally, a few queries marked with N/A result were using unsupported SQL\nsyntax and it was trivial to rewrite them to get proper results. With all of\nthese improvements, we have run ClickBench on QuestDB 6.5.2 and created a\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/ClickHouse/ClickBench/pull/25"},"pull request")," with the\nupdated results."),(0,r.kt)("p",null,"Long story short, although ClickBench has nothing to do with time series\nanalysis, it provided us with a test CSV file and baseline import results, as\nwell as helped us to improve query stability and performance."),(0,r.kt)("h2",{id:"the-import-speed-up"},"The import speed-up"),(0,r.kt)("p",null,"Our new optimized import is based on the SQL ",(0,r.kt)("inlineCode",{parentName:"p"},"COPY")," statement:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-questdb-sql"},"COPY hits FROM 'hits.csv' WITH TIMESTAMP 'EventTime' FORMAT 'yyyy-MM-dd HH:mm:ss';\n")),(0,r.kt)("p",null,"The above command uses the new ",(0,r.kt)("inlineCode",{parentName:"p"},"COPY")," syntax to import the ",(0,r.kt)("inlineCode",{parentName:"p"},"hits.csv")," file from\nClickBench to the ",(0,r.kt)("inlineCode",{parentName:"p"},"hits")," table. For the command to work, the file should be made\navailable in the import root directory configured on the server:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"cairo.sql.copy.root=\\home\\my-user\\my-qdb-import\n")),(0,r.kt)("p",null,"Since we care about time series data analysis, in our experiments, we\npartitioned it by day while the original benchmark used a non-partitioned table.\nLet's start with the most powerful AWS EC2 instance from the original benchmark:"),(0,r.kt)(o.Z,{alt:"Bar chart showing import comparison. From fast to slow: ClickHouse, QuestDB, Apache Pinot, TimescaleDB, DuckDB, and Apache Druid.",title:"Ingesting a 76GB CSV file, from fast to slow: ClickHouse, QuestDB, Apache Pinot, TimescaleDB, DuckDB, and Apache Druid.",height:360,src:"/img/blog/2022-09-12/cover.png",width:650,mdxType:"Screenshot"}),(0,r.kt)("p",null,"The above benchmark compares the import speed of several well-known OLAP and\ntime-series databases: Apache Pinot, Apache Druid, ClickHouse, DuckDB,\nTimescaleDB, and QuestDB. Here, our new optimized ",(0,r.kt)("inlineCode",{parentName:"p"},"COPY")," imports almost 100M\nrows from the ",(0,r.kt)("inlineCode",{parentName:"p"},"hits.csv")," file in 335 seconds, leaving a higher place in the\ncompetition only to ClickHouse."),(0,r.kt)("p",null,"We also did a run on the c6a.4xlarge instance (16 vCPU and 32GB RAM) from the\noriginal benchmark which is noticeably less powerful than the c6a.metal instance\n(192 vCPU and 384GB RAM). Yet, both instances had a rather slow gp2 500GB EBS\nvolume, the result was 17,401 seconds for the less powerful c6a.4xlarge\ninstance. So, in spite of a very slow disk, c6a.metal is 52x faster than\nc6a.4xlarge. Why is that?"),(0,r.kt)("p",null,"The answer is simple. The metal instance has a huge amount of memory, so once\nthe CSV file gets decompressed, it fits fully into the OS page cache. Hence, the\nimport doesn't do any physical reads from the input file and instead reads the\npages from the memory (note: the machine has a\n",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Non-uniform_memory_access"},"NUMA")," architecture,\nbut non-local memory access is still way faster than the disk reads). That's why\nwe observe such huge difference here for QuestDB and, also, you may notice a\n2.5x difference for ClickHouse in the original benchmark."),(0,r.kt)("p",null,"You may wonder why, by removing the need to read the data from the slow disk,\nQuestDB makes a very noticeable improvement, while it's only 2.5x for ClickHouse\nand even less for other databases? We're going to explain it soon, but for now,\nlet's continue the benchmarking fun."),(0,r.kt)("p",null,"Honestly speaking, we find the choice of the metal instance in the ClickBench\nresults rather synthetic, as it makes little sense to use a very powerful (and\nexpensive) machine in combination with a very slow (and cheap) disk. So, we did\na benchmark run on a different test stand:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"c5d.4xlarge EC2 instance (16 vCPU and 32GB RAM), Amazon Linux 2 with 5.15.50\nkernel"),(0,r.kt)("li",{parentName:"ul"},"400GB NVMe drive"),(0,r.kt)("li",{parentName:"ul"},"250GB gp3, 16K IOPS and 1GB/s throughput, or gp2 of the same size")),(0,r.kt)("p",null,"What we got is the following:"),(0,r.kt)(o.Z,{alt:"Bar chart showing QuestDB import performance using different HW.",title:"QuestDB ingestion time for ClickBench's 76GB CSV file by instance type and storage.",height:360,src:"/img/blog/2022-09-12/comparison.png",width:650,mdxType:"Screenshot"}),(0,r.kt)("p",null,"The very last result on the above chart stands for the scenario of c5d.4xlarge\ninstance with a slow gp2 volume. We are including it to show the importance of\nthe disk speed to the performance."),(0,r.kt)("p",null,"In the middle of the chart, the-gp3-volume-only result doesn't use the local\nSSD, but manages to ingest the data into a partitioned table a lot faster than\nthe gp2 run, thanks to the faster EBS volume. Finally, in the NVMe SSD run, the\nimport takes less than 7 minutes - an impressive ingestion rate of 248,000 row/s\n(or 193MB/s) without having the whole input file in the OS page cache. Here, the\nSSD is used as a read-only storage for the CSV file, while the database files\nare placed on the EBS volume. This is a convenient approach for a single-time\nimport of high volume of data. As soon as the import is done, the SSD is no\nlonger needed, so the EBS volume may be attached to a more affordable instance\nwhere the database would run."),(0,r.kt)("p",null,"As shown by the top result in the chart above, the optimized import makes a\nterrific difference for anyone who wants to import their time series data to\nQuestDB, but also takes us close to the ClickHouse's results from the practical\nperspective. Another nice property of QuestDB's import is that, as soon as the\nimport ends, the data is laid out on disk optimally, i.e. the column files are\norganized in partitions and no background merging is required."),(0,r.kt)("p",null,"Now, as we promised, we're going to explain why huge amount of RAM or a\nlocally-attached SSD makes such a difference for QuestDB's import performance.\nTo learn that, we're taking a leap into an engineering story full of trial and\nerror."),(0,r.kt)("h2",{id:"optimizing-the-import"},"Optimizing the import"),(0,r.kt)("p",null,"Our HTTP endpoint, as well as the old ",(0,r.kt)("inlineCode",{parentName:"p"},"COPY")," implementation, is handling the\nincoming data serially (think, as a single-time stream) and uses a single thread\nfor that. For out-of-order (O3) data, this means lots of O3 writes and, hence,\npartition re-writes. Both single-threaded handling and O3 writes become the\nlimiting factor for these types of import."),(0,r.kt)("p",null,"However, the ",(0,r.kt)("inlineCode",{parentName:"p"},"COPY")," statement operates on a file, so there is nothing preventing\nus from going over it as many times as needed."),(0,r.kt)("p",null,"QuestDB's storage format doesn't involve complicated layout like the one in\n",(0,r.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Log-structured_merge-tree"},"LSM trees")," or in other\nsimilar persistent data structures. The column files are\n",(0,r.kt)("a",{parentName:"p",href:"https://questdb.io/docs/concept/partitions"},"partitioned")," by time and versioned\nto handle concurrent reads and writes. The advantages of this approach is that\nas soon as the rows are committed, the on-disk data is optimal from the read\noperation perspective - there is no need to go through multiple files with\npotentially overlapping data when reading from a single partition. The downside\nis that such storage format may be problematic to cope with, when it comes to\ndata import."),(0,r.kt)("p",null,"But no worries, that's something we have optimized."),(0,r.kt)("p",null,"The big ideas we had when working on our shiny new ",(0,r.kt)("inlineCode",{parentName:"p"},"COPY")," are really simple.\nFirst, we should organize the import in multiple phases in order to enable\nin-order data ingestion. Second, we go parallel, i.e. multi-threaded, in each of\nthose phases, where it is possible."),(0,r.kt)("p",null,"Broadly speaking, the phases are:"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Check input file boundaries. Here we try to split the file into N chunks, so\nthat N worker threads may work on their own chunk in isolation."),(0,r.kt)("li",{parentName:"ol"},"Index the input file. Each thread scans its chunk, reads designated timestamp\ncolumn values, and creates temporary index files. The index files are\norganized in partitions and contain sorted timestamps, as well as offsets\npointing to the source file."),(0,r.kt)("li",{parentName:"ol"},"Scan the input file and import data into temporary tables. Here, the threads\nuse the newly built indexes to go through the input file and write their own\ntemporary tables. The scanning and subsequent writes are guaranteed to be\nin-order thanks to the index files containing timestamps and offsets tuples\nsorted by time. The parallelism in this phase comes from multiple partitions\nbeing available to the threads to work independently."),(0,r.kt)("li",{parentName:"ol"},"Perform additional metadata manipulations (say, merge symbol tables) and,\nfinally, move the partitions from temporary tables to the final one. This is\ncompleted in multiple smaller phases that we summarize as one, for the sake\nof simplicity.")),(0,r.kt)("p",null,"The indexes we build at phase 2 may be illustrated in the following way:"),(0,r.kt)(o.Z,{alt:"A diagram showing temporary indexes built during parallel import.",title:"Temporary indexes built during parallel import.",height:265,src:"/img/blog/2022-09-12/diagram.png",width:700,mdxType:"Screenshot"}),(0,r.kt)("p",null,"The above description is an overview of what we've done for the new ",(0,r.kt)("inlineCode",{parentName:"p"},"COPY"),". Yet,\na careful reader might spot a potential bottleneck. Yes, the third phase\ninvolves lots of random disk reads in case of an unordered input file. That's\nexactly what we observed as a noticeable bottleneck when experimenting with the\ninitial implementation. But does it mean that there is nothing we can do with\nthis? Not really. Modern HW & SW to the rescue!"),(0,r.kt)("h2",{id:"io_uring-everything"},"io_uring everything!"),(0,r.kt)("p",null,"Modern SSDs, especially NVMe ones, have evolved quite far from their spinning\nmagnetic ancestors. They're able to cope with much higher concurrency levels for\ndisk operations, including random read ones. But utilizing these hardware\ncapabilities with traditional blocking interfaces, like\n",(0,r.kt)("a",{parentName:"p",href:"https://man7.org/linux/man-pages/man2/pwrite.2.html"},(0,r.kt)("inlineCode",{parentName:"a"},"pread()")),", would involve\nmany threads and, hence, some overhead here and there (like increased memory\nfootprint or context switching). Moreover, QuestDB's threading model operates on\na fixed-size thread pool and doesn't assume running more threads than the\navailable CPU cores."),(0,r.kt)("p",null,"Luckily, newer Linux kernel versions support\n",(0,r.kt)("a",{parentName:"p",href:"https://kernel.dk/io_uring.pdf"},(0,r.kt)("inlineCode",{parentName:"a"},"io_uring")),", a new asynchronous I/O interface.\nBut would it help in our case? Learning the answer is simple and, in fact,\ndoesn't even require a single line of code, thanks to\n",(0,r.kt)("a",{parentName:"p",href:"https://github.com/axboe/fio"},"fio"),", a very flexible I/O tester utility."),(0,r.kt)("p",null,"Let's check how blocking random reads of 4KB chunks would perform on a laptop\nwith a decent NVMe SSD:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ fio --name=read_sync_4k \\\n      --filename=./hits.csv \\\n      --rw=randread \\\n      --bs=4K \\\n      --numjobs=8 \\\n      --ioengine=sync \\\n      --group_reporting \\\n      --runtime=60 \\\n/\n...\nRun status group 0 (all jobs):\n   READ: bw=223MiB/s (234MB/s), 223MiB/s-223MiB/s (234MB/s-234MB/s), io=13.1GiB (14.0GB), run=60001-60001msec\n\nDisk stats (read/write):\n  nvme0n1: ios=3166224/361, merge=0/318, ticks=217837/455, in_queue=218357, util=50.72%\n")),(0,r.kt)("p",null,"Here we're using 8 threads to make blocking read calls to the same CSV file and\nobserve 223MB/s read rate which is not bad at all."),(0,r.kt)("p",null,"Now, we use io_uring to do the same job:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ fio --name=read_io_uring_4k \\\n      --filename=./hits.csv \\\n      --rw=randread \\\n      --bs=4K \\\n      --numjobs=8 \\\n      --ioengine=io_uring \\\n      --iodepth=64 \\\n      --group_reporting \\\n      --runtime=60 \\\n/\n...\nRun status group 0 (all jobs):\n   READ: bw=2232MiB/s (2340MB/s), 2232MiB/s-2232MiB/s (2340MB/s-2340MB/s), io=131GiB (140GB), run=60003-60003msec\n\nDisk stats (read/write):\n  nvme0n1: ios=25482866/16240, merge=6262/571137, ticks=27625314/25206, in_queue=27650786, util=98.86%\n")),(0,r.kt)("p",null,"We get an impressive 2,232MB/s this time. Also, it is worth noting that disk\nutilization has increased to 98.86% against 50.72% in the previous fio run, all\nof that with the same number of threads."),(0,r.kt)("p",null,"This simple experiment proved to us that io_uring may be a great fit in our\nparallel ",(0,r.kt)("inlineCode",{parentName:"p"},"COPY")," implementation, so we added an experimental API and continued\nour experiments. As a result, QuestDB checks the kernel version and, if it's new\nenough, uses io_uring to speed up the import. Our code is also smart enough to\ndetect in-order adjacent lines and read these lines in one I/O operation. Thanks\nto such behavior, parallel COPY is faster than the serial counterpart even on\nordered files."),(0,r.kt)("p",null,"We have explained why presence of a NVMe SSD made such a change in our\nintroductory benchmarks. EBS volumes are very convenient, but they show an order\nof magnitude less IOPS and throughput rates than a physically attached drive.\nThus, using such drive for the purposes of initial data import makes a lot of\nsense, especially when we consider a few terabytes to be imported."),(0,r.kt)("h2",{id:"whats-next"},"What's next?"),(0,r.kt)("p",null,"Prior to QuestDB 6.5, importing large amounts of unsorted data into a\npartitioned table was practically impossible. We hope that our users will\nappreciate this feature, as well as other improvements we've made recently. As a\nlogical next step, we want to take our data import one step further by making it\navailable and convenient to use in QuestDB Cloud. Finally, needless to say,\nwe'll be thinking of more use cases for io_uring in our database."),(0,r.kt)("p",null,"As usual, we encourage you to try out the latest QuestDB 6.5.2 release and share\nyour feedback with our\xa0",(0,r.kt)("a",{parentName:"p",href:"https://slack.questdb.io"},"Slack Community"),". You can also\nplay with our\xa0",(0,r.kt)("a",{parentName:"p",href:"https://demo.questdb.io"},"live demo"),"\xa0to see how fast it executes\nyour queries. And, of course, contributions to our open\nsource\xa0",(0,r.kt)("a",{parentName:"p",href:"https://github.com/questdb/questdb"},"project on GitHub"),"\xa0are more than\nwelcome."))}d.isMDXComponent=!0},86010:function(e,t,n){function a(e){var t,n,i="";if("string"==typeof e||"number"==typeof e)i+=e;else if("object"==typeof e)if(Array.isArray(e))for(t=0;t<e.length;t++)e[t]&&(n=a(e[t]))&&(i&&(i+=" "),i+=n);else for(t in e)e[t]&&(i&&(i+=" "),i+=t);return i}function i(){for(var e,t,n=0,i="";n<arguments.length;)(e=arguments[n++])&&(t=a(e))&&(i&&(i+=" "),i+=t);return i}n.d(t,{Z:function(){return i}})}}]);