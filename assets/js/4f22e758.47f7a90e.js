"use strict";(self.webpackChunkquestdb_io=self.webpackChunkquestdb_io||[]).push([[9732],{3905:function(e,t,n){n.d(t,{Zo:function(){return m},kt:function(){return h}});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),d=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},m=function(e){var t=d(e.components);return a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),c=d(n),h=r,p=c["".concat(l,".").concat(h)]||c[h]||u[h]||o;return n?a.createElement(p,i(i({ref:t},m),{},{components:n})):a.createElement(p,i({ref:t},m))}));function h(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=c;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var d=2;d<o;d++)i[d]=n[d];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},3899:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return l},metadata:function(){return d},toc:function(){return m},default:function(){return c}});var a=n(83117),r=n(80102),o=(n(67294),n(3905)),i=n(72525),s=["components"],l={title:"How we achieved write speeds of 1.4 million rows per second",author:"Vlad Ilyushchenko",author_title:"QuestDB Team",author_url:"https://github.com/bluestreak01",author_image_url:"https://avatars.githubusercontent.com/bluestreak01",description:"Our new ingestion framework can sort time series data before writing to disk. Here's how we built it and how it compares to InfluxDB, ClickHouse, and TimescaleDB.",keywords:["clickhouse","influxdb","timescaledb","tsbs","benchmark","timeseries","database"],image:"/img/blog/2021-05-10/banner.png",tags:["engineering","benchmark","hackernews","clickhouse","timescaledb","influxdb","algorithms"]},d={permalink:"/blog/2021/05/10/questdb-release-6-0-tsbs-benchmark",source:"@site/blog/2021-05-10-questdb-release-6-0-tsbs-benchmark.md",title:"How we achieved write speeds of 1.4 million rows per second",description:"Our new ingestion framework can sort time series data before writing to disk. Here's how we built it and how it compares to InfluxDB, ClickHouse, and TimescaleDB.",date:"2021-05-10T00:00:00.000Z",formattedDate:"May 10, 2021",tags:[{label:"engineering",permalink:"/blog/tags/engineering"},{label:"benchmark",permalink:"/blog/tags/benchmark"},{label:"hackernews",permalink:"/blog/tags/hackernews"},{label:"clickhouse",permalink:"/blog/tags/clickhouse"},{label:"timescaledb",permalink:"/blog/tags/timescaledb"},{label:"influxdb",permalink:"/blog/tags/influxdb"},{label:"algorithms",permalink:"/blog/tags/algorithms"}],readingTime:10.93,truncated:!0,prevItem:{title:"How databases handle 10 million devices in high-cardinality benchmarks",permalink:"/blog/2021/06/16/high-cardinality-time-series-data-performance"},nextItem:{title:"QuestDB version 6.0 alpha",permalink:"/blog/2021/04/20/questdb-release-6-0-alpha"}},m=[{value:"The problem with out-of-order data",id:"the-problem-with-out-of-order-data",children:[]},{value:"How should you store out-of-order time series data?",id:"how-should-you-store-out-of-order-time-series-data",children:[]},{value:"Early thoughts on a solution",id:"early-thoughts-on-a-solution",children:[]},{value:"How we sort, merge, and commit out-of-order time series data",id:"how-we-sort-merge-and-commit-out-of-order-time-series-data",children:[]},{value:"Optimizing copy operations with SIMD",id:"optimizing-copy-operations-with-simd",children:[]},{value:"How often should data be ordered and merged?",id:"how-often-should-data-be-ordered-and-merged",children:[]},{value:"Comparing ingestion with ClickHouse, InfluxDB and TimescaleDB",id:"comparing-ingestion-with-clickhouse-influxdb-and-timescaledb",children:[]},{value:"Adding QuestDB support to the Time Series Benchmark Suite",id:"adding-questdb-support-to-the-time-series-benchmark-suite",children:[]}],u={toc:m};function c(e){var t=e.components,n=(0,r.Z)(e,s);return(0,o.kt)("wrapper",(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"The journey to today's version of QuestDB began with the original prototype in\n2013, and we've described what happened since in a post published during\n",(0,o.kt)("a",{parentName:"p",href:"https://news.ycombinator.com/item?id=23975807"},"our HN launch")," last year. In the\nearly stages of the project, we were inspired by vector-based append-only\nsystems like kdb+ because of the advantages of speed and the simple code path\nthis model brings. We also required that row timestamps were stored in ascending\norder, resulting in fast time series queries without an expensive index."),(0,o.kt)("p",null,"We found out that this model does not fit all data acquisition use cases, such\nas out-of-order data. Although several workarounds were available, we wanted to\nprovide this functionality without losing the performance we spent years\nbuilding."),(0,o.kt)("p",null,"We studied existing approaches, and most came at a performance cost that we\nweren't happy with. Like the entirety of our codebase, the solution that we\npresent today is built from scratch. It took over 9 months to come to fruition\nand adds a further 65k lines of code to the project."),(0,o.kt)("p",null,"Here's what we built, why we built it, what we learned along the way, and\nbenchmarks comparing QuestDB to InfluxDB, ClickHouse and TimescaleDB."),(0,o.kt)("h2",{id:"the-problem-with-out-of-order-data"},"The problem with out-of-order data"),(0,o.kt)("p",null,"Our data model had one fatal flaw - records were discarded if they appear\nout-of-order (O3) by timestamp compared to existing data. In real-world\napplications, payload data doesn\u2019t behave like this because of network jitter,\nlatency, or clock synchronization issues."),(0,o.kt)(i.Z,{alt:"A diagram with two timelines illustrating how data may be delayed from multiple applications to a single database",height:415,src:"/img/blog/2021-05-10/o3-data-illustration.jpg",title:"Out-of-order data in real-world applications",width:650,mdxType:"Screenshot"}),(0,o.kt)("p",null,"We knew that the lack of out-of-order support was a show-stopper for some users\nand we needed a solid solution. There were possible workarounds, such as using a\nsingle table per data source or re-ordering tables periodically, but for most\nusers this is inconvenient and unsustainable."),(0,o.kt)("h2",{id:"how-should-you-store-out-of-order-time-series-data"},"How should you store out-of-order time series data?"),(0,o.kt)("p",null,"As we reviewed our data model, one possibility was to use something radically\ndifferent from what we already had, such as including LSM trees or B-trees,\ncommonly used in time series databases. Adding trees would bring the benefit of\nbeing able to order data on the fly without inventing a replacement storage\nmodel from scratch."),(0,o.kt)("p",null,"What bothered us most with this approach is that every subsequent read operation\nwould face a performance penalty versus having data stored in arrays. We would\nalso introduce complexity by having a storage model for ordered data and another\nfor out-of-order data."),(0,o.kt)("p",null,"A more promising option was to introduce a sort-and-merge phase as data arrives.\nThis way, we could keep our storage model unchanged, while merging data on the\nfly, with ordered vectors landing on disk as the output."),(0,o.kt)("h2",{id:"early-thoughts-on-a-solution"},"Early thoughts on a solution"),(0,o.kt)("p",null,"Our idea of how we could handle out-of-order ingestion was to add a three-stage\napproach:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Keep the append model until records arrive out-of-order"),(0,o.kt)("li",{parentName:"ol"},"Sort uncommitted records in a staging area in-memory"),(0,o.kt)("li",{parentName:"ol"},"Reconcile and merge the sorted data and persisted data at commit time")),(0,o.kt)("p",null,"The first two steps are straightforward and easy to implement, and handling\nappend-only data is unchanged. The heavy commit kicks in only when there is data\nin the staging area. The bonus of this design is that the output is vectors,\nmeaning our vector-based readers are still compatible."),(0,o.kt)("p",null,"This pre-commit sort-and-merge adds an extra processing phase to ingestion with\nan accompanying performance penalty. We nevertheless decided to explore this\napproach and see how far we could reduce the penalty by optimizing the heavy\ncommit."),(0,o.kt)("h2",{id:"how-we-sort-merge-and-commit-out-of-order-time-series-data"},"How we sort, merge, and commit out-of-order time series data"),(0,o.kt)("p",null,"Processing a staging area in bulk gives us a unique opportunity to analyze the\ndata holistically. Such analysis aims to avoid physical ",(0,o.kt)("em",{parentName:"p"},"merges")," altogether\nwhere possible and perhaps get away with fast and straightforward ",(0,o.kt)("inlineCode",{parentName:"p"},"memcpy")," or\nsimilar data movement methods. Such methods can be parallelized thanks to our\ncolumn-based storage. We can employ SIMD and non-temporal data access where it\nmakes a difference."),(0,o.kt)("p",null,"We sort the timestamp column from the staging area via an optimized version of\nradix sort, and the resulting index is used to reshuffle the remaining columns\nin the staging area in parallel:"),(0,o.kt)(i.Z,{alt:"A diagram illustrating how sorting is applied to unordered database records based on a timestamp column order",height:452,src:"/img/blog/2021-05-10/o3-radix-sort.png",title:"Applying sort order to columns in parallel",width:650,mdxType:"Screenshot"}),(0,o.kt)("p",null,"The now-sorted staging area is mapped relative to the existing partition data.\nIt may not be obvious from the start but we are trying to establish the type of\noperation needed and the dimensions of each of the three groups below:"),(0,o.kt)(i.Z,{alt:"A diagram illustrating the combinations of merge operations that can be applied to two data sets",height:400,src:"/img/blog/2021-05-10/staging-area.png",title:"O3 sort and merge scenarios",width:650,mdxType:"Screenshot"}),(0,o.kt)("p",null,"When merging datasets in this way, the prefix and suffix groups can be persisted\ndata, out-of-order data, or none. The merge group is where more cases occur as\nit can be occupied by persisted data, out-of-order data, both out-of-order and\npersisted data, or none."),(0,o.kt)("p",null,"When it's clear how to group and treat data in the staging area, a pool of\nworkers perform the required operations, calling ",(0,o.kt)("inlineCode",{parentName:"p"},"memcpy")," in trivial cases and\nshifting to SIMD-optimized code for everything else. With a prefix, merge, and\nsuffix split, the maximum ",(0,o.kt)("inlineCode",{parentName:"p"},"liveliness")," of the commit (how susceptible it is to\nadd more CPU capacity) is ",(0,o.kt)("inlineCode",{parentName:"p"},"partitions_affected")," x ",(0,o.kt)("inlineCode",{parentName:"p"},"number_of_columns")," x ",(0,o.kt)("inlineCode",{parentName:"p"},"3"),"."),(0,o.kt)("h2",{id:"optimizing-copy-operations-with-simd"},"Optimizing copy operations with SIMD"),(0,o.kt)("p",null,"Because we aim to rely on ",(0,o.kt)("inlineCode",{parentName:"p"},"memcpy")," the most, we benchmarked code that merges\nvariable-length columns:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-cpp"},"template<typename T>\ninline void merge_copy_var_column(\n        index_t *merge_index,\n        int64_t merge_index_size,\n        int64_t *src_data_fix,\n        char *src_data_var,\n        int64_t *src_ooo_fix,\n        char *src_ooo_var,\n        int64_t *dst_fix,\n        char *dst_var,\n        int64_t dst_var_offset,\n        T mult\n) {\n    int64_t *src_fix[] = {src_ooo_fix, src_data_fix};\n    char *src_var[] = {src_ooo_var, src_data_var};\n\n    for (int64_t l = 0; l < merge_index_size; l++) {\n        MM_PREFETCH_T0(merge_index + l + 64);\n        dst_fix[l] = dst_var_offset;\n        const uint64_t row = merge_index[l].i;\n        const uint32_t bit = (row >> 63);\n        const uint64_t rr = row & ~(1ull << 63);\n        const int64_t offset = src_fix[bit][rr];\n        char *src_var_ptr = src_var[bit] + offset;\n        auto len = *reinterpret_cast<T *>(src_var_ptr);\n        auto char_count = len > 0 ? len * mult : 0;\n        reinterpret_cast<T *>(dst_var + dst_var_offset)[0] = len;\n        __MEMCPY(dst_var + dst_var_offset + sizeof(T), src_var_ptr + sizeof(T), char_count);\n        dst_var_offset += char_count + sizeof(T);\n    }\n}\n")),(0,o.kt)("p",null,"with ",(0,o.kt)("inlineCode",{parentName:"p"},"__MEMCPY")," as Angner Fog's Asmlib ",(0,o.kt)("inlineCode",{parentName:"p"},"A_memcpy"),", in one instance and glibC's\n",(0,o.kt)("inlineCode",{parentName:"p"},"memcpy")," in the other."),(0,o.kt)(i.Z,{alt:"A chart showing the performance of memory copy libraries Asmlib and gcliC by megabytes copied over time",height:361,src:"/img/blog/2021-05-10/memcpy-comparison.png",title:"Xeon 8275CL CPU @ 3.00GHz, AVX 512, 3.00GHz, 36608K cache, Amzn2 Linux. Units are microseconds/Mb, lower score is better.",width:650,mdxType:"Screenshot"}),(0,o.kt)(i.Z,{alt:"A chart showing the performance of memory copy libraries Asmlib, gcliC and Windows CRT by megabytes copied over time",height:348,src:"/img/blog/2021-05-10/memcpy-comparison-2.png",title:"i7-3770, 3.40GHz, 8Mb cache, AVX, Ubuntu 20. Units are microseconds/Mb, lower score is better.",width:650,mdxType:"Screenshot"}),(0,o.kt)("p",null,"The key results from this comparison are:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"glibc")," could be slow and inconsistent on AVX512 for our use case. We\nspeculate that ",(0,o.kt)("inlineCode",{parentName:"li"},"A_memcpy")," does better because it uses non-temporal copy\ninstructions."),(0,o.kt)("li",{parentName:"ul"},"Windows ",(0,o.kt)("inlineCode",{parentName:"li"},"memcpy")," is pretty bad."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"A_memcpy")," and ",(0,o.kt)("inlineCode",{parentName:"li"},"memcpy")," perform well on CPUs below AVX512.")),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"A_memcpy")," uses non-temporal streaming instruction which appear to work well\nwith the following simple loop:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-cpp"},"template<typename T>\nvoid set_memory_vanilla(T *addr, const T value, const int64_t count) {\n   for (int64_t i = 0; i < count; i++) {\n       addr[i] = value;\n   }\n}\n")),(0,o.kt)("p",null,"The above is a memory buffer filled with the same 64 bit pattern. It can be\nimplemented as ",(0,o.kt)("inlineCode",{parentName:"p"},"memset")," if all bytes are the same. It also can be written as\nvectorized code which uses platform-specific ",(0,o.kt)("inlineCode",{parentName:"p"},"_mm??_stream_ps(p,?mm)")," in\n",(0,o.kt)("inlineCode",{parentName:"p"},"store_nt")," vector method, as seen below:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-cpp"},"template<typename T, typename TVec>\ninline void set_memory_vanilla(T *addr, const T value, const int64_t count) {\n\n    const auto l_iteration = [addr, value](int64_t i) {\n        addr[i] = value;\n    };\n\n    const TVec vec(value);\n    const auto l_bulk = [&vec, addr](const int64_t i) {\n        vec.store_nt(addr + i);\n    };\n\n    run_vec_bulk<T, TVec>(addr, count, l_iteration, l_bulk);\n}\n")),(0,o.kt)("p",null,"The results were quite surprising. Non-temp SIMD instructions showed the most\nstable results with similar performance to ",(0,o.kt)("inlineCode",{parentName:"p"},"memset"),"."),(0,o.kt)(i.Z,{alt:"A chart showing the performance of non-temporal SIMD instructions, memset and loops for time taken to fill buffers with data",height:409,src:"/img/blog/2021-05-10/non-temporal-memset.png",title:"Initializing a buffer with the same 64bit value",width:650,mdxType:"Screenshot"}),(0,o.kt)("p",null,"Unfortunately, benchmark results with other functions were less conclusive. Some\nperform better with hand-written SIMD and some just as fast with GCC's SSE4\ngenerated code even when it is ran on AVX512 systems."),(0,o.kt)("p",null,"Hand-writing SIMD instructions is both time consuming and verbose. We ended up\noptimizing parts of the code base with SIMD only when the performance benefits\noutweighed code maintenance."),(0,o.kt)("h2",{id:"how-often-should-data-be-ordered-and-merged"},"How often should data be ordered and merged?"),(0,o.kt)("p",null,"While being able to copy data fast is a good option, we think that heavy data\ncopying can be avoided in most time series ingestion scenarios. Assuming that\nmost real-time out-of-order situations are caused by the delivery mechanism and\nhardware jitter, we can deduce that the timestamp distribution will be locally\ncontained by some boundary."),(0,o.kt)("p",null,"For example, if any new timestamp value has a high probability to fall within 10\nseconds of the previously received value, the boundary is then 10 seconds, and\nwe call this boundary ",(0,o.kt)("em",{parentName:"p"},"lag"),"."),(0,o.kt)("p",null,"When timestamp values follow this pattern, deferring the commit can render\nout-of-order commits a normal append operation. The out-of-order system can deal\nwith any variety of lateness, but if incoming data is late within the time\nspecified as ",(0,o.kt)("em",{parentName:"p"},"lag"),", it will be prioritized for faster processing."),(0,o.kt)("h2",{id:"comparing-ingestion-with-clickhouse-influxdb-and-timescaledb"},"Comparing ingestion with ClickHouse, InfluxDB and TimescaleDB"),(0,o.kt)("p",null,"We saw the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/timescale/tsbs"},"Time Series Benchmark Suite"),"\n(TSBS) regularly coming up in discussions about database performance and decided\nwe should provide the ability to benchmark QuestDB along with other systems."),(0,o.kt)("p",null,"The TSBS is a collection of Go programs to generate datasets and then benchmark\nread and write performance. The suite is extensible so that different use cases\nand query types can be included and compared across systems."),(0,o.kt)("p",null,"Here are our results of the benchmark with the ",(0,o.kt)("inlineCode",{parentName:"p"},"cpu-only")," use case using up to\nfourteen workers on an AWS EC2 ",(0,o.kt)("inlineCode",{parentName:"p"},"m5.8xlarge")," instance with sixteen cores."),(0,o.kt)(i.Z,{alt:"Time series benchmark suite results showing QuestDB outperforming ClickHouse, TimescaleDB and InfluxDB when using four workers",height:377,src:"/img/blog/2021-05-10/max-throughput-comparison.png",title:"TSBS results comparing the maximum ingestion throughput of QuestDB, InfluxDB, ClickHouse, and TimescaleDB",width:650,mdxType:"Screenshot"}),(0,o.kt)("p",null,"We reach maximum ingestion performance using four threads, whereas the other\nsystems require more CPU resources to hit maximum throughput. QuestDB achieves\n959k rows/sec with 4 threads. We find that InfluxDB needs 14 threads to reach\nits max ingestion rate (334k rows/sec), while TimescaleDB reaches 145k rows/sec\nwith 4 threads. ClickHouse hits 914k rows/sec with twice as many threads as\nQuestDB."),(0,o.kt)("p",null,"When running on 4 threads, QuestDB is 1.7x faster than ClickHouse, 6.4x faster\nthan InfluxDB and 6.5x faster than TimescaleDB."),(0,o.kt)(i.Z,{alt:"A chart comparing the maximum throughput of four database systems, showing QuestDB hitting ingestion limits with less resources than other systems",height:324,src:"/img/blog/2021-05-10/tsbs-benchmark-results.png",title:"TSBS benchmark results using 4 threads: rows ingested per second by QuestDB, InfluxDB, ClickHouse, and TimescaleDB.",width:650,mdxType:"Screenshot"}),(0,o.kt)("p",null,"Because our ingestion format (ILP) repeats tag values per row, ClickHouse and\nTimescaleDB parse around two-thirds of the total volume of data as QuestDB does\nin the same benchmark run. We chose to stick with ILP because of its widespread\nuse in time series, but we may use a more efficient format to improve ingestion\nperformance in the future."),(0,o.kt)("p",null,"Finally, degraded performance beyond 4 workers can be explained by the increased\ncontention beyond what the system is capable of. We think that one limiting\nfactor may be that we are IO bound as we scale up to 30% better on faster\nAMD-based systems."),(0,o.kt)("p",null,"When we run the suite again using an AMD Ryzen5 processor, we found that we were\nable to hit maximum throughput of 1.43 million rows per second using 5 threads.\nThis is compared to the\n",(0,o.kt)("a",{parentName:"p",href:"https://aws.amazon.com/ec2/instance-types/"},"Intel Xeon\xa0Platinum")," that's in use\nby our reference benchmark ",(0,o.kt)("inlineCode",{parentName:"p"},"m5.8xlarge")," instance on AWS."),(0,o.kt)(i.Z,{alt:"A chart comparing the maximum throughput of QuestDB when utilizing an Intel Xeon\xa0Platinum processor versus an AMD Ryzen5 processor.",height:410,src:"/img/blog/2021-05-10/questdb-bench-amd-ryzen.png",title:"Comparing QuestDB TSBS load results on AWS EC2 using an Intel Xeon\xa0Platinum versus an AMD Ryzen5",width:650,mdxType:"Screenshot"}),(0,o.kt)("h2",{id:"adding-questdb-support-to-the-time-series-benchmark-suite"},"Adding QuestDB support to the Time Series Benchmark Suite"),(0,o.kt)("p",null,"We have opened a pull request\n(",(0,o.kt)("a",{parentName:"p",href:"https://github.com/timescale/tsbs/issues/157"},"#157 - Questdb benchmark support"),")\xa0in\nTimescaleDB's TSBS GitHub repository which adds the ability to run the benchmark\nagainst QuestDB. In the meantime, readers may clone\n",(0,o.kt)("a",{parentName:"p",href:"https://github.com/questdb/tsbs"},"our fork of the benchmark suite")," and run the\ntests to see the results for themselves."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'# Generating the dataset\ntsbs_generate_data --use-case="cpu-only" --seed=123 --scale=4000 \\\n  --timestamp-start="2016-01-01T00:00:00Z" \\\n  --timestamp-end="2016-01-02T00:00:00Z" \\\n  --log-interval="10s" --format="influx" > /tmp/bigcpu\n# Loading the data\ntsbs_load_questdb --file /tmp/bigcpu --workers 4\n')),(0,o.kt)("p",null,"To add out-of-order support, we went for a novel solution that yielded\nsurprisingly good performance versus well-trodden approaches such as B-trees or\nLSM-based ingestion frameworks. We're happy to have shared the journey, and\nwe're eagerly awaiting feedback from the community."),(0,o.kt)("p",null,"For more details, the\n",(0,o.kt)("a",{parentName:"p",href:"https://github.com/questdb/questdb/releases/tag/6.0.0"},"GitHub release for version 6.0"),"\ncontains a changelog of additions and fixes in this release."))}c.isMDXComponent=!0},86010:function(e,t,n){function a(e){var t,n,r="";if("string"==typeof e||"number"==typeof e)r+=e;else if("object"==typeof e)if(Array.isArray(e))for(t=0;t<e.length;t++)e[t]&&(n=a(e[t]))&&(r&&(r+=" "),r+=n);else for(t in e)e[t]&&(r&&(r+=" "),r+=t);return r}function r(){for(var e,t,n=0,r="";n<arguments.length;)(e=arguments[n++])&&(t=a(e))&&(r&&(r+=" "),r+=t);return r}n.d(t,{Z:function(){return r}})}}]);